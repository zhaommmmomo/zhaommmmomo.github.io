{"pages":[{"title":"关于我","text":"这里是don’t want just so so的zhaommmmomo 大四Java渣🐕，准备学C++ 自驱能力强，经常去了解最新的技术和参加中间件比赛🤓 喜欢性能优化、分布式相关，想做基础架构🐱‍🏍 爱好逻辑推理（剧本杀📕、狼人杀🐺、密室逃脱🏃‍♂️） 期待VR！！！✨","link":"/about/index.html"},{"title":"zhaommmmomo个人简历","text":"联系方式 Email：zmm@zhaommmmomo.cn WeChat：zhaommmmomo 个人信息 山东工商学院 · 软件工程 · 本科 （2018 ~ 2022） 技术博客：zhaommmmomo.cn Github：github.com/zhaommmmomo 期望职位：Java开发、中间件开发 期望城市：深圳 关于我 自驱能力较强，能主动去学习新的技术并落地实现。例如：通过LSM树实现一个KV存储引擎、通过Raft算法实现一个集群共识服务以及其他一些算法或数据结构的实现（SkipList / 2pc / 3pc / LRU） 掌握Java（JUC），具备良好的编程规范，并阅读过部分Java源码。了解C / Python 能熟练使用Vertx / SSM / SpringBoot / Redis / MySQL / LevelDB等Java开发相关技术栈，看过Spring部分源码，了解SpringCloud / gRpc / Ignite / RabbitMQ / Nginx等 了解Linux Kerner（网络相关），阅读过其TCP三次握手和网络包收发的实现 熟悉计算机网络 / 操作系统 / 数据结构与算法 / 设计模式 / 计算机组成原理等cs基础知识。了解过TCP的BBR拥塞控制算法。 了解分布式系统 / 系统性能优化 / 微服务相关内容并能熟练使用Jmeter / wrk / Postman / Jprofiler等工具 参加过多次中间件开发比赛（阿里）并且排名都在前1% 项目经验2021.07 ~ 2021.09 分布式聊天室 个人 工作成果： 系统部署在三台ECS服务器上，需要将各个节点联系起来，常见的服务发现机制不足以满足业务需求，最后通过在Ignite上扩展自定义服务发现机制来解决该问题 当用户发送一条消息的时候，会出现集群消息同步以及消息一致性问题，当时考虑了4个方案：Redis、Hazelast、Ignite以及自己实现分布式缓存。最后从系统性能以及实现复杂程度等方面考虑，使用了Ignite的方案来解决此问题。 系统在进行性能测试的时候发现出现连接超时的情况，分析是持久化磁盘IO导致的，通过修改存储方式为LevelDB以及将系统全节点消息同步返回改为多数节点消息同步返回解决了此问题。 2021.03 ~ 2021.05 秒杀系统 个人 工作成果： 用户每次访问秒杀商品加载过程会很慢，通过引入缓存技术，提高了用户访问时界面加载的速度。 由于每次秒杀操作系统都会经历很长的时间，通过使用消息队列异步处理，使用户体验变得更好 在秒杀开始的时候，用户频繁的点击购买按钮，造成大量的请求，我通过将前端购买按钮点击后置灰以及后端限制相同ip每秒的访问次数，大量的减少了系统的压力。 2020.12 ~ 2021.01 搜题库 组长 工作内容： 负责整体系统的设计 负责爬取并处理不同网站上的题库 负责系统核心搜索模块 工作成果： 通过进行数据预处理等手段，将不同网站上爬取到的数据格式进行统一， 使得系统题库数量达到数十万条 通过引入ES搜索引擎，使得系统在数十万条数据中查询的速度为毫秒级。","link":"/resume/index.html"},{"title":"书架","text":"网盘链接的密码都是0000 Reading 理解了再谈网络性能 三体 Linux高性能编程 Redis设计与实现 Effective Java（第三版） 凤凰架构 Good Designing Data-Intensive Applications 深入理解Java虚拟机 Java并发编程的艺术 分布式系统概念与设计（第五版） 深入理解计算机系统 深入理解分布式缓存 剑指offer 中台架构与实现 编程之美 Readed Netty in action Java性能优化权威指南 TCP / IP详解 设计模式就该这样学 Spring5核心原理与30个类手写实战 SpringCloud、Nginx高并发编程 数据库查询优化器的艺术 程序员的三门课 从Paxos到Zookeeper 分布式一致性原理与实践","link":"/books/index.html"},{"title":"论文","text":"LSM-Tree SkipList raft中文版 / raft英文版 WiscKey Chucky From WiscKey to Bourbon BBR Dostoevsky-LSM SlimDB SLM-DB Practical Hash Functions","link":"/paper/index.html"}],"posts":[{"title":"ArrayList底层逻辑与实现","text":"重点扩容规则：底层是System.arraycopy()这个方法 每次扩容都是将新容量设置为1.5 * 旧容量 新容量如果小于需要的容量，将新容量设置为需要的容量 新容量如果大于可以分配的最大容量（整型最大值 - 8），新容量设置为（需要的容量小于最大分配容量）需要的容量或者设置为（需要的容量大于最大分配容量）整型最大值 懒加载机制：ArrayList初始化的时候并没有直接给数组分配容量，而是使用的一个空数组进行赋值，只有当第一次增加的时候才会真正的给数组进行容量分配。这样是为了防止用户初始化后没有进行使用导致内存的浪费。（只有用户调用无参构造方法的时候才是懒加载，其他两个构造函数是分配的另外一个空数组） 属性1234567891011121314// 默认初始容量private static final int DEFAULT_CAPACITY = 10;// 用来表示当用户输入初始容量为0时的数组private static final Object[] EMPTY_ELEMENTDATA = {};// 用来表示是懒加载的数组，与EMPTY_ELEMENTDATA做区分private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {};// 用来存储元素transient Object[] elementData;// 用来记录已经存储了的元素数量private int size; 构造方法我们可以看到我们经常用的List&lt;Integer&gt; arr = new ArrayList&lt;&gt;()，并没有一开始给我们初始化为默认的10容量，而是先赋值的是一个空数组。这里其实就是懒加载的开始， 123456789101112131415161718192021public ArrayList() { // 使用的是懒加载的空数组 this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;}public ArrayList(int initialCapacity) { ...... if (initialCapacity == 0) { // 使用的是非懒加载的空数组 this.elementData = EMPTY_ELEMENTDATA; } ......}public ArrayList(Collection&lt;? extends E&gt; c) { ...... if (size == 0) // 使用的是非懒加载的空数组 elementData = EMPTY_ELEMENTDATA; }} 重要方法add()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public boolean add(E e) { // 用来确定是否需要扩容 ensureCapacityInternal(size + 1); elementData[size++] = e; return true;}private void ensureCapacityInternal(int minCapacity) { ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));}/** * 实现懒加载的方法 */private static int calculateCapacity(Object[] elementData, int minCapacity) { // 判断当前数组是否是懒加载的数组 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { // 如果是的话返回max(默认初始容量，需要的容量) return Math.max(DEFAULT_CAPACITY, minCapacity); } // 如果不是直接返回需要的容量 return minCapacity;}private void ensureExplicitCapacity(int minCapacity) { // 用于记录操作数量 modCount++; // 如果需要的容量小于数组容量，进行扩容 if (minCapacity - elementData.length &gt; 0) grow(minCapacity);}/** * 扩容函数 * 每次在原来基础上一半一半的扩容。 * 如果新容量还是小于需要的容量，将新容量设为需要的容量 * 如果新容量大于最大分配的容量： * 1. 需要的容量也大于。将新容量设置为整型的最大值 * 2. 需要的容量小于，将新容量直接设置为需要的容量 */private void grow(int minCapacity) { int oldCapacity = elementData.length; // 在老容量的基础上增加一半 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) // 这里就是判断需要的容量是否大于可分配容量 // 如果大于就将新容量设置为整型的最大值 // 如果小于就将新容量设置为需要的容量 newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);} 简易实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139/** * @author zmm * @date 2021/11/11 15:12 */public class ArrList&lt;E&gt;{ /** * 默认容量大小 */ private static final int DEFAULT_CAPACITY = 10; /** * 要分配的最大数组大小 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * 空的数组集合，用于懒加载 */ private static final Object[] EMPTY_ELEMENT_DATA = {}; /** * 用来记录已经存放了的数据个数 */ private int size; /** * 用来存放数据 */ private Object[] elementData; public ArrList() { // 注意：这里是给数组赋值的空数组 this.elementData = EMPTY_ELEMENT_DATA; } public ArrList(int initCapacity) { // 这里直接就给数组进行了初始化 if (initCapacity &lt; 0){ throw new IllegalArgumentException(&quot;Illegal Capacity&quot;); } else if (initCapacity == 0){ this.elementData = EMPTY_ELEMENT_DATA; } else { this.elementData = new Object[initCapacity]; } } /** * 添加一个新元素 * @param e 新元素 */ public void add(E e) { lazyLoad(); size++; if (size &gt; elementData.length) { grow(); } elementData[size] = e; } /** * 在指定下标处添加元素 * @param index 下标 * @param element 元素 */ public void add(int index, E element) { if (index &lt; 0 || index &gt; elementData.length) { throw new IllegalArgumentException(&quot;Illegal index&quot;); } lazyLoad(); size++; if (size &gt; elementData.length) { grow(); } // 将原数组下标在index及以后的所有元素复制到以index + 1开始的下标处 System.arraycopy(elementData, index, elementData, index + 1, size - index - 1); elementData[index] = element; } /** * 获取指定下标处的元素 * @param index 下标 * @return 指定下标处的元素 */ @SuppressWarnings(&quot;unchecked&quot;) public E get(int index) { if (index &lt; 0 || index &gt; size){ throw new IllegalArgumentException(&quot;Illegal index&quot;); } return (E) elementData[index]; } /** * 修改指定下标处的元素 * @param index 下标 * @param element 新元素 */ public void set(int index, E element) { if (index &lt; 0 || index &gt; size){ throw new IllegalArgumentException(&quot;Illegal index&quot;); } elementData[index] = element; } /** * 懒加载机制 * 当调用第一次添加的时候才对数组进行初始化操作 * 如果当类加载后直接进行初始化，而用户未使用 * 这样就会导致额外的内存浪费 * 所以这就是为什么刚初始化ArrayList，显示的容量为0 * 添加后就变成了10 */ private void lazyLoad() { if (size == 0){ this.elementData = new Object[DEFAULT_CAPACITY]; } } /** * 扩容函数 * 每次在原来基础上一半一半的扩容。 * 如果新容量还是小于需要的容量，将新容量设为需要的容量 * 如果新容量大于最大分配的容量： * 1. 需要的容量也大于。将新容量设置为整型的最大值 * 2. 需要的容量小于，将新容量直接设置为需要的容量 */ private void grow() { int oldSize = elementData.length; int newSize = oldSize + (oldSize &gt;&gt; 1); if (newSize - size &lt; 0) { newSize = size; } if (newSize &gt; MAX_ARRAY_SIZE){ newSize = size &gt; MAX_ARRAY_SIZE ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; } elementData = Arrays.copyOf(elementData, newSize); }}","link":"/2021/09/11/ArrayList%E5%BA%95%E5%B1%82%E9%80%BB%E8%BE%91%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"title":"Icarus主题的一些常用配置","text":"常见的一些配置见_config.icarus.yml，英文不差都能看懂，或者去看icarus文档。本文主要就是一些网上很少的配置。 Icarus: 4.0.0 文章页面两栏布局在_config.icarus.yml目录下，创建_config.post.yml文件，该文件内容与_config.icarus.yml文件一样，用于单独加载post界面布局。注意，双栏需要将widgets内容的position都设置为同一边。 12345678910111213141516171819202122232425262728# 单独文章界面布局widgets: # 个人信息 - position: left type: profile author: zhaommmmomo author_title: fahaxiki! location: Yantai,China avatar: /img/logo.jpg avatar_rounded: false follow_link: 'https://zhaommmmomo.cn' social_links: Github: icon: fab fa-github url: 'https://github.com/zhaommmmomo' # 文章目录 - position: left type: toc # 目录序号 index: true# 侧边栏是否固定sidebar: left: sticky: false right: sticky: false 增加两栏布局下文章的宽度12345678910111213141516171819202122232425262728# layout/layout.jsxmodule.exports = class extends Component { render() { ...... &lt;Head site={site} config={config} helper={helper} page={page} /&gt; # &lt;body class={`is-${columnCount}-column`}&gt; 修改为下面一行 &lt;body class={`is-3-column`}&gt; ...... # 'is-8-tablet is-8-desktop is-8-widescreen': columnCount === 2 # 修改为下面一行 'is-8-tablet is-8-desktop is-9-widescreen': columnCount === 2, 'is-8-tablet is-8-desktop is-6-widescreen': columnCount === 3 })} dangerouslySetInnerHTML={{ __html: body }}&gt;&lt;/div&gt; ...... }};# layout/common/widgets.jsxfunction getColumnSizeClass(columnCount) { switch (columnCount) { case 2: # return 'is-4-tablet is-4-desktop is-4-widescreen'; # 修改为下面一行 return 'is-4-tablet is-4-desktop is-3-widescreen'; case 3: return 'is-4-tablet is-4-desktop is-3-widescreen'; }} 123456789101112# include/style/responsive.styl+widescreen() # 增加 .is-3-column .container max-width: $widescreen - $gap width: $widescreen - $gap+fullhd() # 增加 .is-3-column .container max-width: $fullhd - 2 * $gap width: $fullhd - 2 * $gap 开启文章目录 在_config.yml中添加toc: true 在_config.icarus.yml或者_config.post.yml中添加 1234567widgets: # 文章目录 - position: left type: toc # 目录序号 index: true 只固定目录123456# source/js/main.jsconst $toc = $('#toc');if ($toc.length &gt; 0) { $toc.addClass('column-left is-sticky'); # 添加 ......} 1234# include/style/widget.styl 添加下面#toc max-height: calc(100vh - 22px) overflow-y: scroll","link":"/2021/09/14/Icarus%E4%B8%BB%E9%A2%98%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"title":"Java中的Thread","text":"前言想必各位都对线程这个词不陌生，我们都知道Java中可以通过Thread、Rannable、Callable和线程池来创建一个线程。但是一个线程的创建、运行到结束到底是一个什么样的过程呢？我们以一段代码为入口点来看看Java到底是如何弄的线程。 12345public static void main(String[] args) { new Thread(() -&gt; { System.out.println(&quot;run...&quot;); }).start();} 源码基本属性1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677// 线程名private volatile String name;// 优先级private int priority;// 没有用到。可能在虚拟机调用的时候会有用。private Thread threadQ;private long eetop;// 是否单步执行此线程private boolean single_step;// 是否是守护线程private boolean daemon = false;// JVM状态private boolean stillborn = false;// run方法实现private Runnable target;// 当前线程的线程组private ThreadGroup group;// 这个线程的上下文类加载器private ClassLoader contextClassLoader;// 线程继承的上下文private AccessControlContext inheritedAccessControlContext;// 用于编号线程名，单调递增private static int threadInitNumber;// 与当前线程相关的ThreadLocal值ThreadLocal.ThreadLocalMap threadLocals = null;// 当前线程继承的ThreadLocal值ThreadLocal.ThreadLocalMap inheritableThreadLocals = null;// 线程的栈大小private long stackSize;// 在本机线程终止后持续存在的 JVM 私有状态。private long nativeParkEventPointer;// 线程Idprivate long tid;// 用于生成线程Idprivate static long threadSeqNumber;// 线程状态private volatile int threadStatus = 0;// 没有用到。可能在虚拟机调用的时候会有用。volatile Object parkBlocker;// 中断标志private volatile Interruptible blocker;// 中断时锁住的对象private final Object blockerLock = new Object();// 最小优先级public final static int MIN_PRIORITY = 1;// 默认优先级public final static int NORM_PRIORITY = 5;// 最大优先级public final static int MAX_PRIORITY = 10;/** * 线程状态 */public enum State { NEW, RUNNABLE, BLOCKED, WAITING, TIMED_WAITING, TERMINATED;} 开始到结束静态代码块12345private static native void registerNatives();static { // 方便调用本地方法块 registerNatives();} init()主要就是初始化线程的一些基本信息（线程名、栈大小、线程组、父线程、优先级、上下文信息等）和安全检测。 注意：这里并没有将线程添加到线程组里面，只是增加线程组中未启动线程的计数。addUnstarted() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394public Thread(Runnable target) { // param1: 线程组 // param2: Runnable // param3: 线程名（从0开始递增） // param4: 分配的栈大小（初始为0） init(null, target, &quot;Thread-&quot; + nextThreadNum(), 0);}private static synchronized int nextThreadNum() { // 从0开始递增 return threadInitNumber++;}private void init(ThreadGroup g, Runnable target, String name, long stackSize) { init(g, target, name, stackSize, null, true);}/** * 线程的初始化方法 * 主要就是初始化线程的一些基本信息（线程名、栈大小、线程组、父线程、优先级等） * @param g 线程组，一般为null，除非有传入。 * @param target Runnable * @param name 线程名。从0开始，单调递增 * @param stackSize 新线程所需的堆栈大小，或为零表示将忽略此参数。 * @param acc 上下文信息。如果为null，就获取当前线程的上下文信息 * @param inheritThreadLocals 是否从构造线程继承可继承线程局部的初始值*/private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc, boolean inheritThreadLocals) { if (name == null) { throw new NullPointerException(&quot;name cannot be null&quot;); } this.name = name; // 获取当前正在执行的线程 Thread parent = currentThread(); // 安全验证 SecurityManager security = System.getSecurityManager(); if (g == null) { if (security != null) { g = security.getThreadGroup(); } if (g == null) { // 就算线程组为null也会设置为父线程的线程组 g = parent.getThreadGroup(); } } // 安全检查 g.checkAccess(); if (security != null) { if (isCCLOverridden(getClass())) { security.checkPermission(SUBCLASS_IMPLEMENTATION_PERMISSION); } } // 增加线程组中未启动线程的计数。未启动的线程不会添加到线程组中， // 以便在它们从未启动时可以收集它们，但必须对其进行计数， // 以便其中包含未启动线程的守护线程组不会被销毁。 g.addUnstarted(); this.group = g; // 会根据父线程是否是守护线程来设置该线程是否为守护线程 this.daemon = parent.isDaemon(); // 根据父线程的优先级来设置该线程的优先级 this.priority = parent.getPriority(); // 设置类加载器 if (security == null || isCCLOverridden(parent.getClass())) this.contextClassLoader = parent.getContextClassLoader(); else this.contextClassLoader = parent.contextClassLoader; // 是否采用当前上下文信息 this.inheritedAccessControlContext = acc != null ? acc : AccessController.getContext(); this.target = target; // 设置优先级 setPriority(priority); // 局部变量 if (inheritThreadLocals &amp;&amp; parent.inheritableThreadLocals != null) this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); // 栈大小 this.stackSize = stackSize; /* Set thread ID */ // 设置线程Id。从0开始单调递增 tid = nextThreadID();} start()首先会判断该线程状态是否是NEW，是的话就将该线程添加到线程组里面（因为在init()方法里面只是增加了计数，并未添加到线程组里面），然后调用Native方法start0()。 123456789101112131415161718192021222324252627282930313233public synchronized void start() { // 0 对应 NEW if (threadStatus != 0) throw new IllegalThreadStateException(); // 将线程添加到线程组中 group.add(this); boolean started = false; try { // 调用native方法。这个方法会修改线程的状态为Running start0(); // 在start0()方法执行完成之后，代表线程已经要开始运行了 // 会直接执行run()方法，而主线程可能还在执行started = true started = true; } finally { try { if (!started) { // 开启失败 group.threadStartFailed(this); } } catch (Throwable ignore) { } }}public void run() { if (target != null) { target.run(); }} exit()被系统方法调用。将该线程从线程组中删除，如果线程组是守护线程组并且没有活动线程并且没有未启动的线程并且没有子线程组，就将该线程组进行销毁。 1234567891011121314151617181920212223242526272829303132333435363738/** * 退出方法。是被系统调用的。 */private void exit() { if (group != null) { // 将该线程从线程组中删除 // 如果该线程组是守护线程组并且活动线程数为0 // 并且未启动的线程为0并且没有子线程组，销毁该线程组 group.threadTerminated(this); group = null; } target = null; threadLocals = null; inheritableThreadLocals = null; inheritedAccessControlContext = null; blocker = null; uncaughtExceptionHandler = null;}void threadTerminated(Thread t) { synchronized (this) { // 将指定线程从线程组中删除。就是从线程数组里面删除该线程 remove(t); // 如果线程组中活动线程数为0，唤醒所有线程 if (nthreads == 0) { notifyAll(); } // 如果是守护线程组并且活动线程数为0并且未启动的线程为0并且没有子线程 if (daemon &amp;&amp; (nthreads == 0) &amp;&amp; (nUnstartedThreads == 0) &amp;&amp; (ngroups == 0)) { // 销毁该线程组 destroy(); } }} Thread状态Thread状态的变化都是通过一些native方法进行改变的。 getState()123456789101112131415161718192021222324public State getState() { // 我们可以看到这里调用的是虚拟机里面的方法 return sun.misc.VM.toThreadState(threadStatus);}// class: sun.misc.VM;/** * 通过&amp;运算符来计算线程的状态。 */public static State toThreadState(int var0) { if ((var0 &amp; 4) != 0) { return State.RUNNABLE; } else if ((var0 &amp; 1024) != 0) { return State.BLOCKED; } else if ((var0 &amp; 16) != 0) { return State.WAITING; } else if ((var0 &amp; 32) != 0) { return State.TIMED_WAITING; } else if ((var0 &amp; 2) != 0) { return State.TERMINATED; } else { return (var0 &amp; 1) == 0 ? State.NEW : State.RUNNABLE; }}","link":"/2021/08/01/Java%E4%B8%AD%E7%9A%84Thread/"},{"title":"Java中的Thread、Rannable和Callable","text":"","link":"/2021/08/03/Java%E4%B8%AD%E7%9A%84Thread%E3%80%81Rannable%E5%92%8CCallable/"},{"title":"Linux中TCP三次握手的实现","text":"前言网上三次握手八股文一大堆，我“为了面试”也去看了看，刚好那时候接触Linux比较多，突然想到TCP三次握手在Linux内核中是如何去实现的呢？是不是会有不同？然后我就开始了漫长的百度（ps: 我比较菜，还不能拿着Linux上千个源码文件去怼）、源码之路。终于弄清了Linux中TCP三次握手的大致过程。 TCP三次握手我那边都知道TCP建立连接前都需要客户端发送一个SYN包，服务端响应一个SYN + ACK包，客户端响应一个ACK包，这是三次握手的基本流程。 但如果让我们去实现三次握手，应该怎么去设计呢？ Linux中的实现我们以网络编程中的相应函数为切入点 Linux中三次握手的主要函数有socket()、bind()、listen()、connect()、recv()、accept() socket()socket 在内核里并不是一个内核对象。而是包含 file、socket、sock 等多个相关内核对象构成，每个内核对象还定义了 ops 操作函数集合。 12// 返回的是一个文件描述符fdfd = socket(AF_INET,SOCK_STREAM, 0); bind()调用bind()函数绑定端口，会使connect()时选择端口方式无效。不推荐在客户端中使用bind()，这会打乱connect的端口选择过程。（但某些情况下可能会bind()，比如说在服务端和客户端进行协商使用什么端口来调用） 流程： 判断用户传入的端口是否小于1024 调用inet_csk_get_port来判断传入端口是否被占用，如果被占用就返回EADDRINUSE。这个方法不会到 ESTABLISH 的哈希表进行可用检测，只在 bind 状态的 socket 里查。所以默认情况下，只要端口用过一次就不会再次使用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135// file: net/ipv4/af_inet.cint inet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len){ struct sockaddr_in *addr = (struct sockaddr_in *)uaddr; struct sock *sk = sock-&gt;sk; struct inet_sock *inet = inet_sk(sk); unsigned short snum; int chk_addr_ret; int err; if (sk-&gt;sk_prot-&gt;bind) { // 如果这个socket有他自己的bind()函数，就使用这个bind()函数 err = sk-&gt;sk_prot-&gt;bind(sk, uaddr, addr_len); goto out; } // ...... // 用户传入的端口号 snum = ntohs(addr-&gt;sin_port); err = -EACCES; // 传入的端口号不允许小于1024 if (snum &amp;&amp; snum &lt; PROT_SOCK &amp;&amp; !capable(CAP_NET_BIND_SERVICE)) goto out; //...... // 尝试确定端口号 // 实际调用的是inet_csk_get_port，用来尝试确定端口号是否被占用 // 如果被占用就返回EADDRINUSE，然后就会显示&quot;Address already in use&quot;(端口被占用) if (sk-&gt;sk_prot-&gt;get_port(sk, snum)) { inet-&gt;inet_saddr = inet-&gt;inet_rcv_saddr = 0; err = -EADDRINUSE; goto out_release_sock; } // ......}// file: net/ipv4/inet_connection_sock.c/** * 获取对给定sock的本地端口的引用， * 如果snum为零，则表示选择任何可用的本地端口。 */int inet_csk_get_port(struct sock *sk, unsigned short snum){ struct inet_hashinfo *hashinfo = sk-&gt;sk_prot-&gt;h.hashinfo; struct inet_bind_hashbucket *head; struct hlist_node *node; struct inet_bind_bucket *tb; int ret, attempts = 5; struct net *net = sock_net(sk); int smallest_size = -1, smallest_rover; local_bh_disable(); // 如果传入的snum不为0 if (!snum) { int remaining, rover, low, high;again: // 获取本地指定的端口范围 inet_get_local_port_range(&amp;low, &amp;high); remaining = (high - low) + 1; // 在端口范围中取一个随机位置开始遍历 smallest_rover = rover = net_random() % remaining + low; smallest_size = -1; do { // 如果本地设置了一个端口不可用 if (inet_is_reserved_local_port(rover)) goto next_nolock; head = &amp;hashinfo-&gt;bhash[inet_bhashfn(net, rover, hashinfo-&gt;bhash_size)]; spin_lock(&amp;head-&gt;lock); inet_bind_bucket_for_each(tb, node, &amp;head-&gt;chain) // 冲突检测，如果端口是用到的，去have_sum逻辑 if (net_eq(ib_net(tb), net) &amp;&amp; tb-&gt;port == rover) { if (tb-&gt;fastreuse &gt; 0 &amp;&amp; sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN &amp;&amp; (tb-&gt;num_owners &lt; smallest_size || smallest_size == -1)) { smallest_size = tb-&gt;num_owners; smallest_rover = rover; if (atomic_read(&amp;hashinfo-&gt;bsockets) &gt; (high - low) + 1) { spin_unlock(&amp;head-&gt;lock); snum = smallest_rover; goto have_snum; } } goto next; } break; next: spin_unlock(&amp;head-&gt;lock); next_nolock: if (++rover &gt; high) rover = low; } while (--remaining &gt; 0); /* Exhausted local port range during search? It is not * possible for us to be holding one of the bind hash * locks if this test triggers, because if 'remaining' * drops to zero, we broke out of the do/while loop at * the top level, not from the 'break;' statement. */ ret = 1; if (remaining &lt;= 0) { if (smallest_size != -1) { snum = smallest_rover; goto have_snum; } goto fail; } /* OK, here is the one we will use. HEAD is * non-NULL and we hold it's mutex. */ snum = rover; } else {have_snum: head = &amp;hashinfo-&gt;bhash[inet_bhashfn(net, snum, hashinfo-&gt;bhash_size)]; spin_lock(&amp;head-&gt;lock); inet_bind_bucket_for_each(tb, node, &amp;head-&gt;chain) if (net_eq(ib_net(tb), net) &amp;&amp; tb-&gt;port == snum) // 判断端口是否被占用 goto tb_found; } // ......} listen()主要是申请和初始化接收队列，包括全连接队列和半连接队列 全连接队列长度：用户传入的 backlog 和 net.core.somaxconn 之间较小的那个值 半连接队列长度：min(backlog, somaxconn, tcp_max_syn_backlog) + 1 再上取整到 2 的幂次，但最小不能小于16。 流程： 根据文件描述符fd查找socket内核对象 获取内核参数net.core.somaxconn，与用户传入的backlog相比较，取最小的 调用sock-&gt;ops-&gt;listen(sock, backlog)函数，实际是inet_listen 判断是否是listen状态 如果不是listen状态，开始监听（接收队列的创建、初始化。内存申请、半连接队列长度的计算、全连接队列头的初始化等）。1. 计算半连接队列长度（与sysctl_max_syn_backlog取 一次最小值；保证不能比8小；向上对齐到2的整数次幂）。2. 申请内存。3. 全队列头初始化，设置为null。4. 将半连接队列挂载到接收队列上。 设置全连接队列的长度（backlog与net.core.somaxconn之间较小的哪个值） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122//file: net/socket.cSYSCALL_DEFINE2(listen, int, fd, int, backlog){ struct socket *sock; int err, fput_needed; int somaxconn; // 根据 fd（文件描述符） 查找对应的 socket 内核对象 sock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed); if (sock) { // 获取内核参数 somaxconn = sock_net(sock-&gt;sk)-&gt;core.sysctl_somaxconn; // 如果设置的backlog大于核心参数 if ((unsigned)backlog &gt; somaxconn) // 使用核心参数 backlog = somaxconn; err = security_socket_listen(sock, backlog); if (!err) // 调用协议栈注册的listen函数 err = sock-&gt;ops-&gt;listen(sock, backlog); fput_light(sock-&gt;file, fput_needed); } return err;}// file: net/ipv4/af_inet.c// 服务端的全连接队列长度是listen时传入的backlog和net.core.somaxconn之间最小的int inet_listen(struct socket *sock, int backlog){ // ...... // 还不是listen状态（没有listen过） if (old_state != TCP_LISTEN) { // 开始监听 err = inet_csk_listen_start(sk, backlog); if (err) goto out; } // 设置全连接队列长度 sk-&gt;sk_max_ack_backlog = backlog; err = 0; // ......}// file: net/ipv4/inet_connection_sock.c// 用来初始化接收队列int inet_csk_listen_start(struct sock *sk, const int nr_table_entries){ // ...... // 用来申请和初始化icsk_accept_queue（半连接队列和全连接队列）这个对象 // 1.定义接收队列数据结构 // 2.接收队列的申请和初始化（内存的申请、半连接队列长度的计算、全连接队列头的初始化） int rc = reqsk_queue_alloc(&amp;icsk-&gt;icsk_accept_queue, nr_table_entries); // ......}// file: net/core/request_sock.c/* * queue：用来存放全连接和半连接队列的结构体 * nr_table_entries：是内核参数和用户传入的backlog中的最小值 * * 1. 定义一个listen_sock指针（半连接队列） * 2. 计算半连接队列的长度，然后申请内存。半连接队列的长度是 min(backlog, somaxconn, tcp_max_syn_backlog) + 1 再上取整到 2 的幂次，但最小不能小于16。 * 3. 将全连接队列头queue-&gt;rskq_accept_head设置成NULL， * 将半连接队列挂载到接收队列queue上 */int reqsk_queue_alloc(struct request_sock_queue *queue, unsigned int nr_table_entries){ size_t lopt_size = sizeof(struct listen_sock); // 半连接队列 struct listen_sock *lopt; // 计算半连接队列的长度 // 再次和sysctl_max_syn_backlog内核对象又取一次最小值 nr_table_entries = min_t(u32, nr_table_entries, sysctl_max_syn_backlog); // 保证nr_table_entries不能比8小。 nr_table_entries = max_t(u32, nr_table_entries, 8); // 用来上对齐到2的整数次幂 // 比如说当前nr_table_entries是最小值8 // 经过roundup_pow_of_two（8+1） = 16 nr_table_entries = roundup_pow_of_two(nr_table_entries + 1); // 为listen_sock对象申请内存，这里包含了半连接队列 lopt_size += nr_table_entries * sizeof(struct request_sock *); if (lopt_size &gt; PAGE_SIZE) lopt = vzalloc(lopt_size); else lopt = kzalloc(lopt_size, GFP_KERNEL); if (lopt == NULL) return -ENOMEM; // 为了效率，不记录nr_table_entries // 而是记录2的几次幂等于nr_table_entries // t for (lopt-&gt;max_qlen_log = 3; (1 &lt;&lt; lopt-&gt;max_qlen_log) &lt; nr_table_entries; lopt-&gt;max_qlen_log++); get_random_bytes(&amp;lopt-&gt;hash_rnd, sizeof(lopt-&gt;hash_rnd)); rwlock_init(&amp;queue-&gt;syn_wait_lock); // 全连接队列头初始化 queue-&gt;rskq_accept_head = NULL; // 半连接队列设置 lopt-&gt;nr_table_entries = nr_table_entries; write_lock_bh(&amp;queue-&gt;syn_wait_lock); queue-&gt;listen_opt = lopt; write_unlock_bh(&amp;queue-&gt;syn_wait_lock); return 0;} connect()流程： 进入内核系统根据用户传入的fd（文件描述符）来查询对应的socket内核对象 调用该sock对象的sock-&gt;ops-&gt;connect方法（inet_stream_connect） 根据sock的状态来进入不同的处理逻辑。第一次connect，sock的状态都是unconnect，所以会调sk-&gt;sk_prot-&gt;connect方法（tcp_v4_connect） 将socket状态设置为TCP-SYN-SENT 动态选择一个端口（首先判断是否bind()了一个端口，如果没有，就根据目标地址和端口等信息生成一个随机数，然后在本地端口范围中通过这个随机数逐渐增加遍历，如果是本地配置的保留端口就跳过，如果不是就遍历已经使用的端口的哈希链表（hinfo-&gt;bhash），判断是否已经被使用。如果该端口已经被使用并且TCP连接中的四元组与当前建立的四元组完全一致，就不可使用。继续遍历，找到了就返回端口，没找到就提示”Address already in use”） 进行tcp连接。（首先申请并设置skb，然后添加到发送队列sk_write_queue上，进行发送，启动重传定时器） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192//file: net/socket.cSYSCALL_DEFINE3(connect, int, fd, struct sockaddr __user *, uservaddr, int, addrlen){ struct socket *sock; struct sockaddr_storage address; int err, fput_needed; // 根据用户fd查找内核中的socket对象 sock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed); // ...... // 调用sock中ops的connect()方法 // 其实是inet_stream_connect() err = sock-&gt;ops-&gt;connect(sock, (struct sockaddr *)&amp;address, addrlen, sock-&gt;file-&gt;f_flags); // ......}//file: ipv4/af_inet.cint inet_stream_connect(struct socket *sock, struct sockaddr *uaddr, int addr_len, int flags){ struct sock *sk = sock-&gt;sk; // ...... switch (sock-&gt;state) { default: err = -EINVAL; goto out; case SS_CONNECTED: err = -EISCONN; goto out; case SS_CONNECTING: err = -EALREADY; /* Fall out of switch with err, set for this state */ break; case SS_UNCONNECTED: // 刚创建完的socket状态就是SS_UNCONNECTED， // 所以第一次connect的就会走这个case err = -EISCONN; if (sk-&gt;sk_state != TCP_CLOSE) goto out; // 调用sock的tcp_v4_connect方法 err = sk-&gt;sk_prot-&gt;connect(sk, uaddr, addr_len); if (err &lt; 0) goto out; // 修改sock的状态 sock-&gt;state = SS_CONNECTING; /* Just entered SS_CONNECTING state; the only * difference is that return value in non-blocking * case is EINPROGRESS, rather than EALREADY. */ err = -EINPROGRESS; break; } // ......}//file: net/ipv4/tcp_ipv4.c// 启动传出连接int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len){ // 设置socket状态为TCP_SYN_SENT tcp_set_state(sk, TCP_SYN_SENT); // 动态选择一个端口 err = inet_hash_connect(&amp;tcp_death_row, sk); // 函数用来根据sk中的信息，构建一个完成的syn报文，并将它发送出去 err = tcp_connect(sk);}//file:net/ipv4/inet_hashtables.c// 绑定一个端口int inet_hash_connect(struct inet_timewait_death_row *death_row, struct sock *sk){ // inet_sk_port_offset(sk): 根据要连接的目的IP和端口等信息生成一个随机数 // __inet_check_established: 检查是否和现有 ESTABLISH 的连接是否冲突的时候用的函数 return __inet_hash_connect(death_row, sk, inet_sk_port_offset(sk), __inet_check_established, __inet_hash_nolisten);}//file:net/ipv4/inet_hashtables.c// 端口号选择int __inet_hash_connect(struct inet_timewait_death_row *death_row, struct sock *sk, u32 port_offset, int (*check_established)(struct inet_timewait_death_row *, struct sock *, __u16, struct inet_timewait_sock **), int (*hash)(struct sock *sk, struct inet_timewait_sock *twp)){ struct inet_hashinfo *hinfo = death_row-&gt;hashinfo; // 是否绑定过端口（bind()方法） const unsigned short snum = inet_sk(sk)-&gt;inet_num; struct inet_bind_hashbucket *head; struct inet_bind_bucket *tb; int ret; struct net *net = sock_net(sk); int twrefcnt = 1; // 如果没有绑定端口 if (!snum) { int i, remaining, low, high, port; static u32 hint; u32 offset = hint + port_offset; struct hlist_node *node; struct inet_timewait_sock *tw = NULL; // 获取本机的端口范围信息 inet_get_local_port_range(&amp;low, &amp;high); remaining = (high - low) + 1; local_bh_disable(); // 遍历查找端口 for (i = 1; i &lt;= remaining; i++) { // 之前算出的随机数+i port = low + (i + offset) % remaining; // 如果本机配置了该端口不可用 if (inet_is_reserved_local_port(port)) continue; // 查找和遍历已经使用的端口的哈希链表 head = &amp;hinfo-&gt;bhash[inet_bhashfn(net, port, hinfo-&gt;bhash_size)]; spin_lock(&amp;head-&gt;lock); inet_bind_bucket_for_each(tb, node, &amp;head-&gt;chain) { // 如果该端口已经被使用 if (net_eq(ib_net(tb), net) &amp;&amp; tb-&gt;port == port) { if (tb-&gt;fastreuse &gt;= 0) goto next_port; WARN_ON(hlist_empty(&amp;tb-&gt;owners)); // 通过 check_established 继续检查是否可用 if (!check_established(death_row, sk, port, &amp;tw)) goto ok; goto next_port; } } } // ......}//file:net/ipv4/tcp_output.c// tcp连接int tcp_connect(struct sock *sk){ struct tcp_sock *tp = tcp_sk(sk); struct sk_buff *buff; int err; // tcp连接初始化 tcp_connect_init(sk); // 申请skb并构造为一个SYN包 buff = alloc_skb_fclone(MAX_TCP_HEADER + 15, sk-&gt;sk_allocation); if (unlikely(buff == NULL)) return -ENOBUFS; // ...... // 添加到发送队列 __tcp_add_write_queue_tail(sk, buff); // ...... // 实际发出syn err = tcp_transmit_skb(sk, buff, 1, sk-&gt;sk_allocation); if (err == -ECONNREFUSED) return err; // ...... /* Timer for repeating the SYN until an answer. */ // 启动重传定时器 inet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS, inet_csk(sk)-&gt;icsk_rto, TCP_RTO_MAX); return 0;} 判断四元组 两对四元组中只要任意一个元素不同，都算是两条不同的连接。以下的两条 TCP 连接完全可以同时存在（假设 192.168.1.101 是客户端，192.168.1.100 是服务端） 连接1：192.168.0.1 5000 192.168.0.2 8090 连接2：192.168.0.1 5000 192.168.0.2 8091 check_established 作用就是检测现有的 TCP 连接中是否四元组和要建立的连接四元素完全一致。如果不完全一致，那么该端口仍然可用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152static int __inet_check_established(struct inet_timewait_death_row *death_row, struct sock *sk, __u16 lport, struct inet_timewait_sock **twp){ // 找到hash桶，所有ESTABLISH状态的socket组成的哈希表。 unsigned int hash = inet_ehashfn(net, daddr, lport, saddr, inet-&gt;inet_dport); spin_lock(lock); /* Check TIME-WAIT sockets first. */ // 遍历查看是否有四元组一样的，一样就报错 sk_nulls_for_each(sk2, node, &amp;head-&gt;twchain) { tw = inet_twsk(sk2); if (INET_TW_MATCH(sk2, net, hash, acookie, saddr, daddr, ports, dif)) { if (twsk_unique(sk, sk2, twp)) goto unique; else goto not_unique; } } tw = NULL; /* And established part... */ // 使用 INET_MATCH 来判断是否可用。 sk_nulls_for_each(sk2, node, &amp;head-&gt;chain) { if (INET_MATCH(sk2, net, hash, acookie, saddr, daddr, ports, dif)) goto not_unique; }unique: // ...... return 0;not_unique: spin_unlock(lock); return -EADDRNOTAVAIL;}// include/net/inet_hashtables.h#define INET_MATCH(__sk, __net, __hash, __cookie, __saddr, __daddr, __ports, __dif) \\ (((__sk)-&gt;sk_hash == (__hash)) &amp;&amp; net_eq(sock_net(__sk), (__net)) &amp;&amp; \\ (inet_sk(__sk)-&gt;inet_daddr == (__saddr)) &amp;&amp; \\ (inet_sk(__sk)-&gt;inet_rcv_saddr == (__daddr)) &amp;&amp; \\ ((*((__portpair *)&amp;(inet_sk(__sk)-&gt;inet_dport))) == (__ports)) &amp;&amp; \\ (!((__sk)-&gt;sk_bound_dev_if) || ((__sk)-&gt;sk_bound_dev_if == (__dif)))) 所以一台客户端机最大能建立的连接数并不是 65535。只要 server 足够多，单机发出百万条连接没有任何问题。 recv()服务器响应SYN主要工作是判断下接收队列是否满了，满的话可能会丢弃该请求，否则发出 synack。申请 request_sock 添加到半连接队列中，同时启动定时器。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158// file: net/ipv4/tcp_ipv4.c// 处理握手过程 int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb){ // ...... // 如果已经建立的TCP连接 if (sk-&gt;sk_state == TCP_ESTABLISHED) { /* Fast path */ sock_rps_save_rxhash(sk, skb-&gt;rxhash); if (tcp_rcv_established(sk, skb, tcp_hdr(skb), skb-&gt;len)) { rsk = sk; goto reset; } return 0; } if (skb-&gt;len &lt; tcp_hdrlen(skb) || tcp_checksum_complete(skb)) goto csum_err; // 服务器收到第一步握手SYN或者第三步ACK都会进入里 if (sk-&gt;sk_state == TCP_LISTEN) { // 进入tcp_v4_hnd_req中查看半连接队列 struct sock *nsk = tcp_v4_hnd_req(sk, skb); if (!nsk) goto discard; if (nsk != sk) { sock_rps_save_rxhash(nsk, skb-&gt;rxhash); if (tcp_child_process(sk, nsk, skb)) { rsk = nsk; goto reset; } return 0; } } else sock_rps_save_rxhash(sk, skb-&gt;rxhash); // 根据不同的socket状态进行不同的处理 if (tcp_rcv_state_process(sk, skb, tcp_hdr(skb), skb-&gt;len)) { rsk = sk; goto reset; } // ......}// file:net/ipv4/tcp_input.c// 除了 ESTABLISHED 和 TIME_WAIT，其他状态下的 TCP 处理都走这里int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb, struct tcphdr *th, unsigned len){ // ...... // 第一次握手或第三次握手，服务器收到ack包 case TCP_LISTEN: if (th-&gt;ack) // 如果是响应包 return 1; if (th-&gt;rst) goto discard; if (th-&gt;syn) { // 如果是SYN握手包 // conn_request是个函数指针，指向tcp_v4_conn_request // 服务器响应 SYN 的主要处理逻辑都在这个tcp_v4_conn_request里 if (icsk-&gt;icsk_af_ops-&gt;conn_request(sk, skb) &lt; 0) return 1; kfree_skb(skb); return 0; } // ......}// file: net/ipv4/tcp_ipv4.c// 响应SYN的主要处理逻辑都在这个方法中// 1.先判断一下半连接队列是否满了// 如果满了就进入tcp_syn_flood_action（syn flood攻击）查看是否开启tcp_syncookies内核参数，没有开启就会丢弃该握手包// 2.判断全连接队列是否满了// 如果满了并且如果有young_ack，直接丢弃// young_ack（未处理完的半连接请求）是半连接队列里保持着的一个计数器。// 记录的是刚有SYN到达，没有被SYN_ACK重传定时器重传过SYN_ACK,// 同时也没有完成过三次握手的sock数量// 3.申请request_sock分配内核对象// 4.构造syn+ack包并发送响应，tcp_v4_send_synack()方法中。// 5.添加到半连接队列中并开启计时器重传。int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb){ // ...... // 判断半连接队列是否满了，如果满了就进入syn_flood_warning去判断是否开启 // 了 tcp_syncookies 内核参数。 // 如果队列满，且未开启 tcp_syncookies，那么该握手包将直接被丢弃 if (inet_csk_reqsk_queue_is_full(sk) &amp;&amp; !isn) if (net_ratelimit()) syn_flood_warning(skb); // ...... // 在全连接的情况下，如果有young_ack，那么直接丢 // young_ack 是半连接队列里保持着的一个计数器。 // 记录的是刚有SYN到达，没有被SYN_ACK重传定时器重传过SYN_ACK, // 同时也没有完成过三次握手的sock数量 if (sk_acceptq_is_full(sk) &amp;&amp; inet_csk_reqsk_queue_young(sk) &gt; 1) goto drop; // 分配request_sock内核对象 req = inet_reqsk_alloc(&amp;tcp_request_sock_ops); if (!req) goto drop; // ...... // tcp_v4_send_synack()构造并发送syn+ack响应 if (tcp_v4_send_synack(sk, dst, req, (struct request_values *)&amp;tmp_ext) || want_cookie) goto drop_and_free; // 添加到半连接队列，并开启计时器 // 计时器的作用是在某个时间之内还收不到客户端的第三次握手， // 服务器就会重传syn+ack包 inet_csk_reqsk_queue_hash_add(sk, req, TCP_TIMEOUT_INIT); // ......}// file: net/ipv4/tcp_ipv4.c // 构造并发送syn + ackstatic int tcp_v4_send_synack(struct sock *sk, struct dst_entry *dst, struct request_sock *req, struct request_values *rvp){ // ...... // 构造syn + ack包 skb = tcp_make_synack(sk, dst, req, rvp); if (skb) { // 如果构建成功 __tcp_v4_send_check(skb, ireq-&gt;loc_addr, ireq-&gt;rmt_addr); // 发送syn + ack响应 err = ip_build_and_send_pkt(skb, sk, ireq-&gt;loc_addr, ireq-&gt;rmt_addr, ireq-&gt;opt); err = net_xmit_eval(err); } dst_release(dst); return err;} 客户端响应SYN + ACK清除了 connect 时设置的重传定时器，把当前 socket 状态设置为 ESTABLISHED，开启保活计时器后发出第三次握手的 ack 确认。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131// file:net/ipv4/tcp_input.cint tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb, struct tcphdr *th, unsigned len){ // ...... // 客户端第二次握手处理 case TCP_SYN_SENT: // 处理 syn+ack 包 queued = tcp_rcv_synsent_state_process(sk, skb, th, len); if (queued &gt;= 0) return queued; /* Do step6 onward by hand. */ tcp_urg(sk, skb, th); __kfree_skb(skb); tcp_data_snd_check(sk); return 0; }}// file: net/ipv4/tcp_input.cstatic int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb, struct tcphdr *th, unsigned len){ // ...... if (th-&gt;ack) { // ...... // 修改socket状态 tcp_set_state(sk, TCP_ESTABLISHED); // 连接建立完成 security_inet_conn_established(sk, skb); // ...... // 初始化拥塞控制 tcp_init_congestion_control(sk); // ...... // 保活计时器打开 if (sock_flag(sk, SOCK_KEEPOPEN)) inet_csk_reset_keepalive_timer(sk, keepalive_time_when(tp)); // ...... if (sk-&gt;sk_write_pending || icsk-&gt;icsk_accept_queue.rskq_defer_accept || icsk-&gt;icsk_ack.pingpong) { /* Save one ACK. Data will be ready after * several ticks, if write_pending is set. * * It may be deleted, but with this feature tcpdumps * look so _wonderfully_ clever, that I was not able * to stand against the temptation 8) --ANK */ // 延迟确认 inet_csk_schedule_ack(sk); icsk-&gt;icsk_ack.lrcvtime = tcp_time_stamp; icsk-&gt;icsk_ack.ato = TCP_ATO_MIN; tcp_incr_quickack(sk); tcp_enter_quickack_mode(sk); inet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK, TCP_DELACK_MAX, TCP_RTO_MAX);discard: __kfree_skb(skb); return 0; } else { // 申请构造ack包然后返回响应 tcp_send_ack(sk); } return -1; } /* No ACK in the segment */ if (th-&gt;rst) { /* rfc793: * &quot;If the RST bit is set * * Otherwise (no ACK) drop the segment and return.&quot; */ goto discard_and_undo; } // ...... if (th-&gt;syn) { /* We see SYN without ACK. It is attempt of * simultaneous connect with crossed SYNs. * Particularly, it can be connect to self. */ // 如果是syn包，就设置socket状态为TCP_SYN_RECV tcp_set_state(sk, TCP_SYN_RECV); // ...... // 序号seq+1 tp-&gt;rcv_nxt = TCP_SKB_CB(skb)-&gt;seq + 1; tp-&gt;rcv_wup = TCP_SKB_CB(skb)-&gt;seq + 1; // ...... // 发送syn + ack包 tcp_send_synack(sk); } // ......}// file:net/ipv4/tcp_output.cvoid tcp_send_ack(struct sock *sk){ // ...... // 申请和构造ack包 buff = alloc_skb(MAX_TCP_HEADER, GFP_ATOMIC); // ...... // 发送ack包 tcp_transmit_skb(sk, buff, 0, GFP_ATOMIC);} 服务端响应ACK把当前半连接对象删除，创建了新的 sock 后加入到全连接队列中，最后将新连接状态设置为 ESTABLISHED。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105// 服务端第三次握手的ack与第一次握手一样，都会进入到tcp_v4_do_rcv，此时去半连接队列中查看就不是空的了，会保留第一次握手的半连接信息// file:net/ipv4/tcp_ipv4.cstatic struct sock *tcp_v4_hnd_req(struct sock *sk, struct sk_buff *skb){ // ...... // 查找listen socket的半连接队列 struct request_sock *req = inet_csk_search_req(sk, &amp;prev, th-&gt;source, iph-&gt;saddr, iph-&gt;daddr); if (req) // 如果找到了 return tcp_check_req(sk, skb, req, prev); // ......}// file：net/ipv4/tcp_minisocks.c// 主要是创建一个子socket，然后清理半连接队列，添加到全连接队列中struct sock *tcp_check_req(struct sock *sk, struct sk_buff *skb, struct request_sock *req, struct request_sock **prev){ // ...... // 创建子socket // 对应的是tcp_v4_syn_recv_sock 函数 child = inet_csk(sk)-&gt;icsk_af_ops-&gt;syn_recv_sock(sk, skb, req, NULL); if (child == NULL) goto listen_overflow; // 清理半连接队列 inet_csk_reqsk_queue_unlink(sk, req, prev); inet_csk_reqsk_queue_removed(sk, req); // 添加全连接队列 inet_csk_reqsk_queue_add(sk, req, child); return child; // ......}// file:net/ipv4/tcp_ipv4.c// 创建sock内核对象struct sock *tcp_v4_syn_recv_sock(struct sock *sk, struct sk_buff *skb, struct request_sock *req, struct dst_entry *dst){ // ...... // 判断接收队列是否满了，如果满了就修改一下计数器然后丢弃 if (sk_acceptq_is_full(sk)) goto exit_overflow; // 创建sock &amp;&amp; 初始化 newsk = tcp_create_openreq_child(sk, req, skb); // ......}// file: include/net/inet_connection_sock.h static inline void inet_csk_reqsk_queue_unlink(struct sock *sk, struct request_sock *req, struct request_sock **prev){ // 把连接请求块从半连接队列中删除 reqsk_queue_unlink(&amp;inet_csk(sk)-&gt;icsk_accept_queue, req, prev);}// file:net/ipv4/syncookies.cstatic inline void inet_csk_reqsk_queue_add(struct sock *sk, struct request_sock *req, struct sock *child){ // 将握手成功的request_sock对象插入到全连接队列链表的尾部 reqsk_queue_add(&amp;inet_csk(sk)-&gt;icsk_accept_queue, req, sk, child);}// file:net/ipv4/tcp_input.cint tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb, struct tcphdr *th, unsigned len){ // ...... switch (sk-&gt;sk_state) { case TCP_SYN_RECV: //第三次握手处理 if (acceptable) { tp-&gt;copied_seq = tp-&gt;rcv_nxt; smp_mb(); // 修改状态为已连接 tcp_set_state(sk, TCP_ESTABLISHED); // ...... } else { return 1; } break; } // ......} accept()服务端accept：从已建立好的全连接队列中取第一个返回给用户进程 123456789101112131415161718192021222324252627282930313233343536373839// file: net/ipv4/inet_connection_sock.cstruct sock *inet_csk_accept(struct sock *sk, int flags, int *err){ // ...... // 从全连接队列中获取第一个元素 newsk = reqsk_queue_get_child(&amp;icsk-&gt;icsk_accept_queue, sk); // ......}// file:include/net/request_sock.hstatic inline struct sock *reqsk_queue_get_child(struct request_sock_queue *queue, struct sock *parent){ // 从全连接队列中获取第一个元素 struct request_sock *req = reqsk_queue_remove(queue); struct sock *child = req-&gt;sk; WARN_ON(child == NULL); sk_acceptq_removed(parent); __reqsk_free(req); return child;}// file:include/net/request_sock.hstatic inline int reqsk_queue_removed(struct request_sock_queue *queue, struct request_sock *req){ struct listen_sock *lopt = queue-&gt;listen_opt; if (req-&gt;retrans == 0) --lopt-&gt;qlen_young; return --lopt-&gt;qlen;} 总结握手前的准备 客户端：通过socket()方法获取一个fd文件描述符，通过bind()方法绑定一个端口（可以不调用该方法；如果调用了，首先会判断用户 传入的端口号是否大于1024，然后通过inet_csk_get_port方法判断该端口是否被占用，如果被占用就返回``EADDRINUSE`。只会在bind状态的socket里面查找，不会去ESTABLISH 的哈希表进行可用检测） 服务端：通过socket()方法获取一个fd文件描述符，通过listen()方法初始化连接队列（全连接队列大小 = min(backlog, net.core.somaxconn)，半连接队列大小 = min(backlog, somaxconn, tcp_max_syn_backlog) + 1 再向上取整到 2 的幂次，但最小不能小于16） 第一次握手 客户端：根据传入的fd文件描述符，找到对应的socket内核对象。调用sock-&gt;ops-&gt;connect方法（inet_stream_connect），然后在tcp_v4_connect()方法中将socket状态设置为TCP_SYN_SENT，通过_inet_hash_connect()方法动态选择一个端口号（首先判断是否bind()了一个端口，如果没有，就根据目标地址和端口等信息生成一个随机数，然后在本地端口范围中通过这个随机数逐渐增加遍历，如果是本地配置的保留端口就跳过，如果不是就遍历已经使用的端口的哈希链表（hinfo-&gt;bhash），判断是否已经被使用，如果该端口已经被使用并且TCP连接中的四元组与当前建立的四元组完全一致，就不能使用，继续遍历。找到了就返回端口，没找到就提示Address already in use）。通过tcp_connect()构建skb并添加到发送队列sk_write_queue上，启动重传定时器，进行发送。 第二次握手 服务端：首先会判断半连接队列是否满了，如果满了就进入syn_flood_warning去判断是否开启了 tcp_syncookies 内核参数。如果队列满，且未开启 tcp_syncookies，那么该握手包将直接被丢弃，然后去判断一下全连接队列是否满了，如果满了并且有young_ack，直接丢弃，否则发出 synack。申请request_sock 添加到半连接队列中，同时启动定时器。young_ack（未处理完的半连接请求）是半连接队列里保持着的一个计数器。记录的是刚有SYN到达，没有被SYN_ACK重传定时器重传过SYN_ACK,同时也没有完成过三次握手的sock数量。 客户端：清除重传定时器，将当前socket状态设置为ESTABLISHED，开启保活计时器后发出ack确认 第三次握手 服务端：找到半连接队列中的request_sock对象，判断全连接队列是否满了，如果满了就修改计数器然后丢弃，没满就创建新的sock对象，将半连接中的连接请求进行删除，添加到全连接队列中，将状态设置为TCP_ESTABLISHED。 最后 调用accept()方法从全连接队列中获取一个返回给用户进程","link":"/2021/07/11/Linux%E4%B8%ADTCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E7%9A%84%E5%AE%9E%E7%8E%B0/"},{"title":"SpringIOC大致流程","text":"是什么？​ 官方文档的解释是：IoC也称为依赖注入（DI）。在此过程中，对象仅通过构造函数参数，工厂方法的参数或在构造或从工厂方法返回后在对象实例上设置的属性来定义其依赖项（即，与它们一起使用的其他对象） 。然后，容器在创建bean时注入那些依赖项。从本质上讲，此过程是通过使用类的直接构造或诸如服务定位器模式之类的机制来控制其依赖关系的实例化或位置的Bean本身的逆过程（因此，其名称为Control Inversion）。 ​ 简单来说：就是我们将一个个的bean对象交给IoC去管理，他会帮助我们去创建对象实例、填充属性、初始化、添加监听器等过程。 类图​ 我们以常用的ClassPathXmlApplicationContext为例 大致过程 ​ 首先，一个IoC容器应创建一个工厂（DefaultListableBeanFactory），可以使我们读取的资源文件可以存放。 ​ 然后，将配置文件通过一个规范（BeanDefinitionReader）加载出来。 ​ 接着，是bean对象实例化之前的一些准备（初始化啊、事件处理器、注册组件等）；例如上图中的BeanFactoryPostProcessor、多播器等。 ​ 重要的地方来了，创建一个个的非懒加载的成品Bean对象（finishBeanFactoryInitialization方法）。 ​ 最后，是一些事件的发布、缓存、销毁等。 源码分析​ 从ClassPathXmlApplicationContext开始分析。在它的构造方法中，我们可以看见调用了父类（AbstractApplicationContext类）的构造方法、设置配置文件的加载路径以及核心方法refresh()方法。 ​ 父类AbstractApplicationContext的构造方法 ​ setConfigLocations()方法 ​ 接下来，我们进入核心方法**refresh()**。 ​ 我们重点看序号2和序号11，其他有兴趣可以自己点进去看看。 ​ obtainFreshBeanFactory()方法 ​ ​ 跟进refreshBeanFactory()方法，在AbstractRefreshableApplicationContext类中可以找到refreshBeanFactory()这个方法 ​ createBeanFactory()方法中 ​ loadBeanDefinitions()方法，也是委派给子类去实现。 我们进去子类AbstractXmlApplicationContext类的loadBeanDefinition()方法。在这里进行了配置文件读取规范的定义，我们继续跟进loadBeanDefinitions()方法。 loadBeanDefinitions()方法。传入的可能是个String[]或者Resource[]类型。但是大致流程都差不多：String[]-&gt;String-&gt;Resource[]-&gt;Resource-&gt;Document-&gt;BeanDefinition。这里就不过多深入了，感兴趣可以照这个流程看下去。 资源文件加载完成后，我们的BeanFactory差不多就创建好了。接着，我们到IoC最重要的过程，Bean对象（不是懒加载的）的实例化和初始化。这里为什么将实例化和初始化分开说呢，是想更好的帮助理解Bean对象的创建过程。其实Spring中更加的细分了一下，分成了实例化（createBeanInstance()方法）、填充属性（populateBean()方法）和初始化（initializeBean()方法）。 ​ 实例化：在堆中开辟了一块空间。属性都是系统默认值。 ​ 初始化：给属性完成具体的赋值操作，调用具体的初始化方法。 ​ 好了，我们进入finishBeanFactoryInitialization()方法，里面你会看到一些对beanFactory的属性设置，其中重点的是preInstantiateSingletons()方，点进去，它会调用DefaultListableBeanFactory的preInstantiateSingletons()方法。 我们可以看到**getBean()**方法，这里就是准备开始进行bean对象的创建了。 点进去，我们可以看真正执行的是**doGetBean()**方法 doGetBean()方法，就是根据不同的Bean采用不同的创建策略。 如果Bean是单例的，则在容器创建之前先从缓存中查找，确保整个容器只存在一个实例对象 如果Bean是原型模式的，则容器每次都会创建一个新的实例对象 指定了Bean的生命周期 ​ 我们进入createBean()，发现还有一个doCreateBean方法()，终于，我们到了真正创建Bean对象的方法。点进去。 ​ 我们发现我们终于找到了之前所说的那三个方法了，创建、填充和初始化。 ​ createBeanInstance()方法返回的是一个BeanWrapper，bean的封装类。 ​ populateBean()则是将bean的一些属性字段进行解析、填充。 ​ 在initializeBean()中 到此，我们一开始的流程图所有的地方差不多都完成了。其中有些细节方面没点进去看看，主要是大致了解IoC的过程。可以自行debug进去看看。","link":"/2021/04/23/SpringIOC%E5%A4%A7%E8%87%B4%E6%B5%81%E7%A8%8B/"},{"title":"SpringMVC大致过程","text":"MVCMVC是一种框架模式，将M和V的实现代码分离。 M（Model）：模型，业务规则。处理请求、返回数据，数据可以被多个视图使用。 V（View）：视图，就是你能看到并能交互的界面。 C（Controller）：控制器，负责接收用户的请求去调用哪个M去处理，然后再返回确定哪个V显示数据。 Servlet为什么在这里讲一下Servlet？其实SpringMVC就是一个Servlet，所以弄清楚Servlet的过程，SpringMVC的就类似了。 我们现来看一下Servlet的类图关系。 SpringMVCSpringMVC和Servlet的关系 大致过程 初始化的时候用了一个Map保存URL和Controller类的对应关系 根据请求的URL找到对应的Controller，然后从Controller中找到处理请求的方法 将请求参数绑定到方法的形参上，执行方法处理请求，并返回结果视图 九大组件在DispathcherServlet类中，我们可以找到SpringMVC的九大组件。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * 九大组件之一 * 用于处理上传请求。将普通请求封装 */@Nullableprivate MultipartResolver multipartResolver;/** * 九大组件之一：本地语言解析器 * 从请求中解析出Local。主要用于i18n。 */@Nullableprivate LocaleResolver localeResolver;/** * 九大组件之一：模板解析器 * 负责从请求中解析出主题名，主题就是样式、图片以及它们所形成的显示效果的集合。相关类： * ThemeSource：根据主题名查找具体的主题 */@Nullableprivate ThemeResolver themeResolver;/** * 九大组件之一 * 用来查找Handler。可以是类也可以是方法。 * 比如标注了@RequestMapping的每个方法都可以看成一个Handler */@Nullableprivate List&lt;HandlerMapping&gt; handlerMappings;/** * 九大组件之一：适配器 * 因为Servlet的方法结构都是doService(HttpServletRequest req, * HttpServletResponse resp)形式的， * 将SpringMVC中的Handler转为这种格式。 */@Nullableprivate List&lt;HandlerAdapter&gt; handlerAdapters;/** * 九大组件之一：异常处理器 * 处理Handler产生的异常情况的组件 */@Nullableprivate List&lt;HandlerExceptionResolver&gt; handlerExceptionResolvers;/** * 九大组件之一 * 从请求中获取ViewName */@Nullableprivate RequestToViewNameTranslator viewNameTranslator;/** * 九大组件之一 * 用于管理FlashMap。FlashMap用于重定向时的参数传递 */@Nullableprivate FlashMapManager flashMapManager;/** * 九大组件之一：视图解析器 * 将String类型的视图名和Locale解析为View类型的视图，只有一个resolveViewName()方法。 */@Nullableprivate List&lt;ViewResolver&gt; viewResolvers; 源码解析SpringMVC初始化首先我们从Servlet的初始化init()开始，在DispatcherServlet类中，我们并没有发现Servlet的初始化方法，所以我们去父类中去找找。在HttpServletBean中我们终于发现了init()方法。 我们可以看见这里只是对一些初始的属性参数进行了设置，具体通过子类调用initServletBean()这个方法来调用。我们点进去看看 我们可以看到子类FrameworkServlet的initServletBean()方法中添加了一些日志相关的信息，以及一个重要的方法**initWebApplicationContext()**，这个方法最终会调用我们IoC容器的refresh()方法。剩下的initFrameworkServlet()这个方法是个空方法，可以留给我们进行扩展。我们继续跟进initWebApplicationContext()方法。 我们进入createWebApplicationContext()方法中 configureAndRefreshWebApplicationContext()方法，终于我们看见了refresh()方法 其实就是IoC容器的初始化。 我们接着看SpringMVC九大组件的初始化 在DispatcherServlet中，onRefresh()方法和initStrategies()方法 到此，SpringMVC初始化差不多就完成了，下面去看看SpringMVC是如何处理请求的。 SpringMVC处理请求在doService()方法中。其中最核心的方法是doDispatch()方法。 我们进入doDispatch()。 这里面的处理大致流程是： 先判断一下是不是文件上传 通过getHandler()方法找到请求对应的处理handler的Bean实例以及添加一些拦截器，封装成一个HandlerExecutionChain请求处理链对象 获取一个支持的处理适配器HandlerAdapter。getHandlerAdapter() 调用给定的handler处理请求，执行并返回结果视图。handle() 处理一下处理的结果，可以是ModelAndView或者是一个异常。processDispatchResult() 我们先去看看getHandler()方法。 继续点进去，在AbstractHandlerMapping的getHandler()方法中。 是不是看见了一个我们很熟悉的方法？getBean()。这时候我们就获取到了对应handler的实例了，然后就是调用getHandlerExecutionChain()方法，将一些拦截器与之封装为一个HandlerExecutionChain。 这时候一个HandlerExecutionChain就完成了。 然后我们去看看getHandlerAdapter()方法。 接下来是handle()方法，我们点进去看看，他会调用AbstractHandlerMethodAdapter类的handle方法，我们会发现它最后会调用handleInternal()这个方法，交给子类去实现。 进入handleInternal()方法 我们可以看见都会调用invokeHandlerMethod()这个方法，跟进去。都是些什么addXXX、setXXX、registerXXX方法。最重要的是invokeAndHandle()方法，它完成了将请求中的参数和方法中的参数进行绑定，通过@RequestParam注解或者是ASM框架。 invokeAndHandle()方法 然后就是返回相应的ModelAndView了。 接下来对ModelAndView进行处理，在processDispatchResult()方法中。可以看见对异常的处理和对正常视图的处理。 其中调用了render()方法对视图进行渲染。 到此，一个请求的处理差不多就完成了。SpringMVC的工作也差不多结束了。其中还有些细节的地方，比如在将请求中的参数和方法的参数绑定时ASM框架的使用，感兴趣可以去看看。 关于调用了refresh()方法，九大组件什么时候初始化。 前面我们知道看初始化的时候，如果进行了refresh()，下面的onRefresh()方法好像就不能调用了啊，那九大组件到底是什么时候初始化的呢？ 这其实用到了事件监听器。我们都知道IoC容器的refresh()方法中有事件传播器的注册（initApplicationEventMulticaster()方法）、事件的发布（finishRefresh()）方法。其实就是在事件发布的时候，调用了SpringMVC九大组件的初始化。我们后续再讲。","link":"/2021/04/26/SpringMVC%E5%A4%A7%E8%87%B4%E8%BF%87%E7%A8%8B/"},{"title":"Spring中Bean的生命周期","text":"Bean是什么？本来没有这一节的，但是写完源码之后，在想是否能够类比一下别的更具体的东西，然后想着想着突然想到了个Bean到底是什么？好像用了这么久的Spring，都还没想过这个问题。然后就去看看官网、书、博客等。就有了这一节。 官网的解释： 《Spring5核心原理与30个类手写实战》中的解释:Bean对于Spring的意义就像Object对于OOP的意义一样。Spring在Java组件化（JavaBean、EJB等）开发理念下出现的。 个人理解：Bean是一个组件（对象），组成了我们的应用程序，通过IoC可以对其进行管理。就好比去吃自助餐里面的一道道菜。 生命周期源码中的描述： 大致过程： 源码分析上图中的核心方法几乎都在doCreateBean()中，我们直接进入到AbstractAutowireCapableBeanFactory类中。 首先，进入bean实例的创建，createBeanInstance()方法。 我们可以看见上图中Bean可以通过instantiateUsingFactoryMethod()方法创建，也可以通过autowireConstructor()方法创建，但是默认的是使用instantiateBean()方法创建。我们点进去看看 我们可以看到这里面有JDK的安全API以及对获取到的bean实例对象进行封装，最重要的是getInstantiationStrategy().instantiate();这个方法。我们继续点进去。 是不是看见了个很熟悉的方法？clazz.getDeclaredConstructor();获取构造方法。是不是接下来就会想到了对应的newInstance()？这其实就是在BeanUtils.instantiateClass(constructorToUse);调用了。到此，Bean的实例化出来了，后续就是我们上面说的对Bean实例的封装BeanWrapper了。 接下来，我们进入bean实例对象的属性填充（populateBean()方法）。 重要的是applyPropertyValues()这个方法，我们点进去看看。 其中最重要的两个方法是：解析resolveValueIfNecessary()和注入setPropertyValues()方法。 大致过程是： 属性类型不需要强制转换时，不需要解析属性值，直接进行依赖注入 属性值类型需要进行强制转换时，如对其他对象的引用等，首先需要解析属性值，然后对解析后的属性值进行依赖注入 我们进入resolveValueIfNecessary()方法，可以看见这里面有对不同类型属性的解析，感兴趣自己点进去看看。 我们进入setPropertyValues()这个方法，到AbstractPropertyAccessor这个类中，我们会发现很多setPropertyValues的重载方法。但是都会走到**setPropertyValue()**这个方法中。 我们继续跟进，还是会发现有很多的setPropertyValue()方法的重载，我靠，怎么这么多！但最后还是会回到这几个方法。这时候，我们属性的依赖注入终于要进行了。我们可以看见主要有3类，对数组类型、对List类型和对Map类型的注入 到此，我们的属性填充终于终于终于完成了，重载的方法是真的多！ 接下来，我们进入初始化方法（initializeBean()）中。 在这个里面，我们可以看见上面画的流程图中的大部分方法了。Aware接口、BeanPostProcess和init-method Aware接口上面已经贴出来了，就不展示了。 applyBeanPostProcessorsBeforeInitialization()方法 invokeInitMethods()方法 applyBeanPostProcessorsAfterInitialization()方法 终于要到销毁方法了，在refresh()中的destroyBeans()方法。 我们点进去，会调用当前BeanFactory的destroySingletons()方法 调用父类的destroySingletons()方法 到此，我们整个Bean的生命周期就结束了。","link":"/2021/04/24/Spring%E4%B8%ADBean%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/"},{"title":"【比赛复盘】云上开发，高效智能–阿里云ECS Cloudbuild开发者大赛性能挑战赛道","text":"前言快暑假的时候，在阿里天池上面闲逛。诶，性能优化挑战赛？点进去看看，初略的读了一下赛题，要实现一个聊天室服务，部署在ECS服务器上面，然后对它进行一下手段的性能优化，感觉还挺合适的（ps: 当时因为写了一个秒杀系统并压测优化过、也看过一些性能调优的书，感觉自己很nb，后面才知道自己还是太嫩了🤣），顿时就报名了。 因为第一次参加这种比赛，所以给自己定的是前50就ok了。然后一个月的初赛，快结束的那一周排名十多名，感觉自己还是很厉害的，但是就一周的时间，看着自己排名直接跌出前 20，到了 23。才意识到自己高兴太早了。 接下来是一个月的复赛时间，这回是要实现一个集群了。中途遇到了老多问题，找了好多官网文档（Vertx、Hazelcast、Ignite、LevelDB），也学到了不少，但还是感觉差点火候😅。每天早上七点多起床一直调试到晚上十点左右，官方人员都被烦的不要不要的了😂（希望不要厌烦）。还是太菜了哈哈哈。 不多说了，先上排名。 赛题背景：基于公共云构建产品、系统和应用已经是当前最热门的技术趋势了。赛题探讨Web Service在云上部署的性能优化，希望参赛者通过代码撰写、操作系统与数据库选型、各种参数调优等手段，优化云端Web服务的性能和保障服务的高可用。 描述：根据给定的API完成：用户注册、用户登录、创建房间、查看房间、用户进入、退出实时聊天房间、用户发言和实时收取其他人的消息。注：聊天室、消息和用户，三个数据必须持久化；在线人数无须持久化。需要保证程序返回时宕机，数据还存在，允许很少的丢失。三台服务器。 机器规格：4 核 8 G 评分：能测试的综合得分= n ∗ 10 + 50 ∗ (qps / 10000) ∗ k + 50 ∗ (1 / time_deplay) ∗ m。 优化点线程模型一开始系统的架构采用的是Springboot + redis（用了MySQL，直接没出分😅）。写完程序后提交，纳尼？居然才拿到了简单的API正常分（190）？立马打开Jprofiler开始监控，发现压测的时候线程数只有几十个并且线程大多都处于等待状态，而且CPU利用率很低。ok，提高tomcat的线程数（因为Springboot是内嵌tomcat的，而且一个请求会有一个线程去处理），不断的调整，最后找到最优的线程数。鸡冻！立马打开提交界面开始提交，过了几分钟，QPS得分提高 5 分？？？怎么才 5 分？我测了这么久！算了算了，提高了就好。 但是接下来一段时间，陷入艰难的时刻了，不断的修改、提交，但是分数老是上不去。不行不行，不能就这样下去，大概在网上找了好几天，期间都差点想去学C++的seastar，后面在techempower网站上找到了性能较高的web容器Vertx，终于看见了曙光。花了 3 ~ 5 天去熟悉Vertx一个个的demo，后面根据需要然后不断的找文档、加功能，一个webserver弄好了！（不得不说，Vertx的文档是我见过最容易懂的，学的最快的）先点个提交看看，300+！芜湖。 同步模型像传统的Springboot就是同步模型（其实是tomcat），收到一个请求后，就用一个线程去处理，只能处理一个，其他线程就可能会被阻塞，这样就会大大的降低系统的吞吐量。虽然tomcat后面也支持NIO以及AIO，但是在比赛初期的时候使用了但是效果不太明显，就舍弃了。 多路复用模型像我们知道的Linux中的select、poll和epoll，又或者是Redis中的，都是使用的IO多路复用。Vertx正是使用了这种模型，它通过一个Event Loop组不断的循环，当有Event到达时，它会调用相应的handle去处理。这就使得它相比于同步模型有着天然的性能优势，能够处理更多的请求。注意：不要在Event Loop运行时间较长的代码，Vertx有专门用于处理耗时长的代码线程（Worker Verticle）。 线程数我们前面说到Vertx是通过一个Event Loop组来不断的循环以及Worker Verticle来处理阻塞代码，但是它们的线程数量应该如何去设置呢？我的思路：Event Loop因为是不断循环的，线程数应和CPU核数差不多。而Worker Verticle处理阻塞代码，可以尽可能的多。最终经过不断的benchmark，调整线程数，最终提高的大概 50 多分。 12345678910111213// file: io.vertx.core.Launcher@Overridepublic void beforeStartingVertx(VertxOptions options) { // 设置EventLoop线程池大小 options.setEventLoopPoolSize(8);}@Overridepublic void beforeDeployingVerticle(DeploymentOptions deploymentOptions) { // 设置部署多少个Verticle实例 deploymentOptions.setInstances(100);} 数据写入这是另外一个大头。因为以前接触存储这方面比较少，当时认知里面Redis最快，在使用的时候一直开启aof的AOF_FSYNC_ALWAYS，因为赛题要求系统返回就意味着数据罗盘。后面公布可以允许少量的数据丢失后，开启了AOF_FSYNC_EVERYSEC，分数提高了 100 分左右。然后在网上参考一些写性能快的中间件（可能是我没搜引擎之类的，居然没看见LevelDB），后面不知道是哪个博客上面给的测试报告，说MongoDB的写性能优于Redis，然后花了好几天的时间去弄MongoDB和测试，但是在 4 核 8 G的情况下，内存占用太多，并且性能也没Redis好。然后参考了MongoDB的实现，异步定时刷盘。后面就在想自己弄个存储的，直接写，不调中间件了，整个方案是写在内存中然后到达某一阈值 / 时限后异步刷盘，但是测试出来效果都不理想，就给放弃了。后面就是乱找了，像Cassandra、ScyllaDB这些（当时居然没想到看底层是如何实现的了）。 后面初赛完才了解到LevelDB以及LSM树。详情见：LSM论文 集群（通信 / 数据传输）这其实都还挺好弄的，主要是一般以上的节点 ack true 后 response。当时考虑的方案有 通过分布式缓存实现。像Redis集群，又或者嵌入在程序里面的Hazelcast、Ignite。 自己实现 但是当时因为在其他方案上面浪费了很多时间，所以就选择了成熟的实现较快的方案：Ignite。其他方面的原因就是当时想着嵌入在应用程序里面比像Redis额外开一个进程资源消耗低以及网络IO少。但是为什么不选择Hazelcast呢？因为这个东西持久化缓存的话需要 money ！！！ 像这种简单、需求不多、追求性能的场景感觉还是自己实现并且在一个程序里面好点。 至于数据传输的格式，protobuf就挺不错的，兼容性和性能都挺好。其他的像Json、Xml用的比较多。 比赛完后实现了一个通过Java实现Raft算法，具体详情移步到该文章。 数据分区刚拿到复赛赛题的时候，一开始考虑的是集群每个节点都保存一份完整的数据（ps: 当时想着快速响应，当本地内存中存有数据时，这样最快），后面测试过程中发现查询的效率是挺快的了，但是当集群面临写多的场景下，就出现了很多问题。比如说消息顺序乱（考虑的是时间戳来解决）、集群之间数据同步太慢（主要原因）。后面还是将数据进行了分区存放，集群中每个节点都会存储一块区域的数据，当本地分区没有该数据时，就会去其他分区查询，并可以按需要将查询到的数据缓存到本地，这样做后性能提高了不少。 但是随之而来的问题就是当存储某一分区的节点宕机之后，其他节点如何访问该分区的数据？ 后面在Ignite官网查看到了数据再平衡机制。就是会有个中心的节点会管理分区，并将分区分配给集群中的节点，当某一节点宕机后，中心节点就会将宕机后的分区数据流转到其他两个节点上面，这样就解决的数据丢失问题。（因为评测环境不会出现三台服务器都宕机的场景，所以不管如何宕机数据都不会丢失，前提是宕机后的节点重启会先进行数据同步） Linux可打开的文件数这东西感觉还挺常用的哈，具体来说就是连接数的大小，因为Linux都是以fd文件描述符来弄的嘛，所以设置Linux中可以打开的最大文件数可以应对一些connection refuse的场景。具体配置： 12345678# vi /etc/sysctl.conf# 设置系统级别的fs.file-max=1100000fs.nr_open=1100000# sysctl -p 设置生效# vi /etc/security/limits.confsoft nofile 1010000hard nofile 1010000 注意：hard nofile 一定要比 fs.nr_open 要小，否则可能会导致用户无法登录 总结 一个好的架构 / 设计（这会帮你省下很多时间） 技术的广度与深度（你可以快速的找到你所需要的技术） 学习能力（能够快速熟练的上手某个新的技术） 心态（放轻松） 加油！！！ 最后复赛第二技术分享","link":"/2021/11/07/%E3%80%90%E6%AF%94%E8%B5%9B%E5%A4%8D%E7%9B%98%E3%80%91%E4%BA%91%E4%B8%8A%E5%BC%80%E5%8F%91%EF%BC%8C%E9%AB%98%E6%95%88%E6%99%BA%E8%83%BD%E2%80%93%E9%98%BF%E9%87%8C%E4%BA%91ECS%20Cloudbuild%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B%E6%80%A7%E8%83%BD%E6%8C%91%E6%88%98%E8%B5%9B%E9%81%93/"},{"title":"动态规划","text":"前言​ 前一个月开始刷题，那会儿在力扣上面，见一个DP题一个不会的。突然觉得自己好垃圾哈哈哈。为什么现在写这个呢，就刚刚刷着刷着题，突发奇想了，觉得好像找到了哪个感觉？开始刷DP过瘾了哈哈哈。但是我又不太会讲，直接来分享我对一些DP案例题的解题思路。不多说，进入正题。 案例(其他几个案例整理出来后补上)青蛙跳级问题 一个青蛙一次可以跳上1级台阶，也可以跳上2级台阶。求该青蛙跳上一个n级台阶总共有多少种跳法。 思考 青蛙跳级的方式有2种： 跳1个台阶 跳2个台阶 将问题转变为数学函数： 解题 123456789101112131415class Solution { public int climbStairs(int n) { int[] res = new int[n+1]; if(n &lt;= 0) return 0; if(n == 1) return 1; if(n == 2) return 2; res[0] = 0; res[1] = 1; res[2] = 2; for(int i = 3; i &lt;= n; i++){ res[i] = res[i-1] + res[i-2]; } return res[n]; }} 拓展 如果小青蛙有三种跳法呢？跳一个台阶、跳两个台阶、跳三个台阶呢？ 斐波那契数列 力扣72题 背包问题 假如你有一个可以装5kg的背包，你去商城里面得将最有价值的物品装进背包，但是不能超过背包容量。求你能带走的最大价值的总和 商场中的物品有： 物品 重量 价值 铅笔 0.5kg 1 面包 1kg 3 手电筒 3kg 10 水 2kg 7 思考 平常我们遇到这种问题，都是想着先把性价比最高的拿了，然后拿第二的，但是在某些场景下就不太适合了。这里就不太过多讲解，主要使用动态规划来解这道题目。 物品 0.5kg 1kg 2kg 3kg 4kg 5kg 铅笔 1 2 4 6 8 10 面包 1 3 6 9 12 15 手电筒 1 3 6 10 13 16 水 1 3 7 10 14 17 主要思路：将每一阶段重量的最优解保存 例如： 当只有铅笔的时候： 0.5kg的最优解是1、1kg的最优解是2……5kg的最优解是10 当有铅笔和面包的时候：0.5kg的最优解是1、1kg的最优解是3……5kg的最优解是15 …… 当全部物品都在时：0.5kg的最优解是1、1kg的最优解是3……5kg的最优解是17 我们这时就能得到最终答案为17","link":"/2021/04/04/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"title":"循环依赖","text":"是什么？简单的来说就是对象a的属性中引用了对象b，对象b的属性中引用了对象c……最后引用到a。 1234567891011&lt;bean id=&quot;a&quot; class=&quot;com.zmm.test.A&quot; lazy-init=&quot;false&quot;&gt; &lt;property name=&quot;b&quot; ref=&quot;b&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;b&quot; class=&quot;com.zmm.test.B&quot; lazy-init=&quot;false&quot;&gt; &lt;property name=&quot;c&quot; ref=&quot;c&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;c&quot; class=&quot;com.zmm.test.C&quot; lazy-init=&quot;false&quot;&gt; &lt;property name=&quot;a&quot; ref=&quot;a&quot;/&gt;&lt;/bean&gt; Spring是如何解决的？1. 三级缓存（其实二级缓存也能解决，只是看是否使用AOP）1234567891011121314151617181920212223242526// DefaultSingletonBeanRegistry类下的/** * 一级缓存 * 用来存放成品bean对象（已经成功实例化与初始化了的） * * Cache of singleton objects: bean name to bean instance. */private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;&gt;(256);/** * 三级缓存 （AOP的关键，如果不用AOP，二级缓存也能解决循环依赖） * 用来存放早期的bean实例(lambda表达式，一个匿名内部类的形式)，看bean对象是否需要被代理。 * ObjectFactory&lt;?&gt;是一个函数式接口， * * Cache of singleton factories: bean name to ObjectFactory. */private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap&lt;&gt;(16);/** * 二级缓存 * 用来存放半成品bean对象，已经实例化了的但是未初始化的bean对象 * * Cache of early singleton objects: bean name to bean instance. */private final Map&lt;String, Object&gt; earlySingletonObjects = new ConcurrentHashMap&lt;&gt;(16); 2.提前暴露AbstractAutowireCapableBeanFactory类的doCreateBean()方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253protected Object doCreateBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException { // ....... // Eagerly cache singletons to be able to resolve circular references // even when triggered by lifecycle interfaces like BeanFactoryAware. // 急于缓存单例，以便即使在诸如BeanFactoryAware之类的生命周期接口触发时也能够解析循环引用。 // 是否是单例的 &amp;&amp; 允许循环引用 &amp;&amp; 是Singleton当前bean正在创作中 boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) { if (logger.isTraceEnabled()) { logger.trace(&quot;Eagerly caching bean '&quot; + beanName + &quot;' to allow for resolving potential circular references&quot;); } // 一个匿名内部类，提前暴露创建的实例bean。可以防止循环引用，尽早的持有对象的引用 // 如果一级缓存中不存在指定的bean，就添加到三级缓存中去，将二级缓存中的移除 addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)); } // ........................ // 如果是提前暴露的单例 if (earlySingletonExposure) { // 获取指定名称的已注册的单例模式的Bean对象 Object earlySingletonReference = getSingleton(beanName, false); if (earlySingletonReference != null) { // 如果获取到的bean对象不为空 if (exposedObject == bean) { // 如果获取到的Bean对象与当前实例化的Bean对象相同 // 将实例暴露出去，当前实例初始化完成 exposedObject = earlySingletonReference; } // 当前Bean依赖其他Bean，并且当发生循环引用时不允许创建新的实例对象 else if (!this.allowRawInjectionDespiteWrapping &amp;&amp; hasDependentBean(beanName)) { String[] dependentBeans = getDependentBeans(beanName); Set&lt;String&gt; actualDependentBeans = new LinkedHashSet&lt;&gt;(dependentBeans.length); // 获取当前Bean所依赖的其他Bean for (String dependentBean : dependentBeans) { if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) { actualDependentBeans.add(dependentBean); } } // ....... } } } // ...... return exposedObject;} BeanDefinitionValueResolver类下的resolveValueIfNecessary()方法 1234567891011// 解析属性值，对注入类型进行转换public Object resolveValueIfNecessary(Object argName, @Nullable Object value) { // 对引用类型的属性进行解析 if (value instanceof RuntimeBeanReference) { RuntimeBeanReference ref = (RuntimeBeanReference) value; // 调用引用类型的解析方法 return resolveReference(argName, ref); } // ......对其他类型的属性的解析} 大致流程 ​ 源码分析A引用B、B引用C、C引用A。根据上面大致流程来。在docreate()方法中，先对a实例化 将a实例的引用暴露出去 准备去填充属性，进入populateBean()方法，applyPropertyValues()方法继续进入，重点来了，resolveValueIfNecessary()方法 继续进入，我们会看到会调用resolveReference()这个方法 最终又会调用**getBean()**方法 接下来我就省略对b的实例化，直接去看对c的。 继续跟着上面的走，在BeanDefinitionValueResolver类的resolveReference()方法时，调用getBean()去获取了a的实例 从缓存中获取a的实例 我们继续跟进到c实例的doGetBean()方法 这时候，c的实例化初始化已经完成了，然后就是b的初始化以及a的初始化了，步骤类似，自行去debug吧。 细节1、在三级缓存中，实例的变更情况。例如a、b、c。序号代表顺序 一级缓存：7.添加c、9.添加b、11.添加a 二级缓存：4.添加a、10.移除a 三级缓存：1.添加a、2.添加b、3.添加c、5.移除a、6.移除c、8.移除b 2、关于构造器注入和set注入，下面是官网的解释","link":"/2021/04/22/%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96/"},{"title":"通过Java实现Raft算法","text":"前言7月份的时候参加了一个阿里天池的性能优化比赛，后面在复赛的时候因为是集群场景，需要考虑各个节点之间数据的一致性，本来想自己实现的，但奈何时间太短（ps: 太菜了😭），最终还是找了市面上成熟的中间件来实现（Ignite）。这不，还是手痒，自己实现一个基于Raft的一致性服务。 Github：zraft 个人博客：zhaommmmomo Raft是什么？Raft是一个为了管理复制日志的一致性算法。它提供和Paxos算法相同的功能和性能，但是它的算法结构与Paxos不同并且更加易于理解 Raft 通过选举一个Leader，然后给予他全部的管理复制日志的责任来实现一致性。领导人从客户端接收日志条目（log entries），把日志条目复制到其他服务器上，并告诉其他的服务器什么时候可以安全地将日志条目应用到他们的状态机中。数据的流向只能是Leader -&gt; otherNode。 状态机 复制状态机通常都是基于复制日志实现的，如图 1。每一个服务器存储一个包含一系列指令的日志，并且按照日志的顺序进行执行。每一个日志都按照相同的顺序包含相同的指令，所以每一个服务器都执行相同的指令序列。因为每个状态机都是确定的，每一次执行操作都产生相同的状态和同样的序列。 一致性算法的任务是保证复制日志的一致性。服务器上的一致性模块接收客户端发送的指令然后添加到自己的日志中。它和其他服务器上的一致性模块进行通信来保证每一个服务器上的日志最终都以相同的顺序包含相同的请求，即使有些服务器发生故障。一旦指令被正确的复制，每一个服务器的状态机按照日志顺序处理他们，然后输出结果被返回给客户端。因此，服务器集群看起来形成了一个高可靠的状态机。 实际系统中使用的一致性算法通常含有以下特性： 安全性保证（绝对不会返回一个错误的结果）：在非拜占庭错误情况下，包括网络延迟、分区、丢包、重复和乱序等错误都可以保证正确。 可用性：集群中只要有大多数的机器可运行并且能够相互通信、和客户端通信，就可以保证可用。因此，一个典型的包含 5 个节点的集群可以容忍两个节点的失败。服务器被停止就认为是失败。它们稍后可能会从可靠存储的状态中恢复并重新加入集群。 不依赖时序来保证一致性：物理时钟错误或者极端的消息延迟只有在最坏情况下才会导致可用性问题。 通常情况下，一条指令可以尽可能快的在集群中大多数节点响应一轮远程过程调用时完成。小部分比较慢的节点不会影响系统整体的性能。 概念节点状态 Leader：负责处理所有Client请求，并将entries通过AppendEntries()RPC方法添加到其他节点去。 Candidate：可以变为Leader的节点。当某一段时间内没有收到心跳或者收到的大多数票数时，就会变为Leader，给其他节点发送心跳。否则变为Follower Follower：只响应来自其他服务器的请求。集群刚启动时，所有节点状态都是Follower，当某一段时间内没有收到其他节点的信息，就会变为Candidate并向其他节点请求投票。 任期（term） Raft将任期（term）作为逻辑时间。任期自增的整数表示（初始为0）。每一段任期从一次选举开始，如果一个候选人赢得选举，然后他就在接下来的任期内充当领导人的职责。在某些情况下，一次选举过程会造成选票的瓜分。在这种情况下，这一任期会以没有领导人结束；一个新的任期（和一次新的选举）会很快重新开始。Raft 保证了在一个给定的任期内，最多只有一个领导人。 选举方法（RequestVote()） 如果term &lt; currentTerm返回 false 如果 votedFor 为空或者为 candidateId，并且候选人的日志至少和自己一样新，那么就投票给他 1234567891011121314151617181920public VoteResponse requestVote(VoteRequest voteRequest) { // 如果term &lt; currentTerm返回 false // 如果 votedFor 为空或者为 candidateId，并且候选人的日志至少和自己一样新，那么就投票给他}Class VoteRequest { /** 候选人的任期号 */ long term; /** 候选人Id */ long candidateId; /** 候选人的最后日志条目的索引值 */ long lastLogIndex; /** 候选人最后日志条目的任期号 */ long lastLogTerm}class VoteResponse { /** 当前节点的任期号 */ long term; /** 是否投票 */ boolean voteGranted;} 集群刚启动的时候，所有节点都是Follower状态。如果Follower在选举超时内每收到心跳或者投票请求，它就会进行选举投票，先增加自己的任期号并转换为Candidate，然后向其他节点发送RPC投票请求。 获得了大多数的选票。修改状态为Leader，修改维护的nextIndex[]数组为当前日志条目的索引，关闭等待超时计时器，开启心跳计时器并发送心跳包。 其他节点成为Leader。如果Leader的任期号不小于当前任期号，修改状态为Follower。 出现同票情况。随机生成超时时间后重新开始新一轮的选举。 追加条目（AppendEntries()）只能由Leader -&gt; 其他节点，不能到Leader，是单向的。 Client发送RPC请求，Leader首先会将日志追加到本地，追加失败则返回false。然后通过AppendEntries()方法同步到其他节点上去，当Leader收到大多数节点响应true时，会将该日志条目Commit，然后将结果返回给Client，然后通知其他节点Commit。 当追加的条目为空时，代表这是个心跳包 如果当前任期大于请求任期，返回false 如果当前日志条目没有能够与preLogIndex和preLogTerm匹配的，返回false 重置等待计时器等待时间 如果发生条目冲突（索引相同，任期不同），删除冲突索引以后的所有日志 追加日志条目 如果Leader的commitIndex大于本地的，将本地的设置为min(commitIndex. logIndex) 123456789101112131415161718192021222324252627282930313233public AppendResponse appendEntries(AppendRequest appendRequest) { // 如果当前任期大于请求任期，返回false // 如果当前日志条目没有能够与preLogIndex和preLogTerm匹配的，返回false // 重置等待计时器等待时间 // 如果发生条目冲突（索引相同，任期不同），删除冲突索引以后的所有日志 // 追加日志条目 // 如果Leader的commitIndex大于本地的，将本地的设置为min(commitIndex. logIndex)}class AppendRequest { /** Leader的任期号 */ long term; /** LeaderId */ long leaderId; /** 新日志的前一个日志条目的索引 */ long preLogIndex; /** 新日志的前一个日志条目的任期号 */ long preLogTerm; /** 需要添加的条目信息 */ List&lt;Entry&gt; entries; /** Leader的提交索引 */ long leaderCommit;}class Entry { long term; /** 命令 */ String command;}class AppendResponse { /** 当前节点的任期号 */ long term; /** Follower的条目是否与Leader的匹配上了 */ boolean success;} Leader对于每个Follower都维护 一个nextIndex，记录需要给该Follower发送的下一个日志条目的索引。当某一个节点刚成为Leader时，它会将所有nextIndex设置为自己的最后一个日志的index + 1。如果一个Follower的日志和Leader不一致，那么在下一次的AppendEntries() RPC 时的一致性检查就会失败。在被Follower拒绝之后，Leader就会减小 nextIndex 值并进行重试。最终 nextIndex 会在某个位置使得Leader和Follower的日志达成一致。当这种情况发生，附加日志 RPC 就会成功，这时就会把Follower冲突的日志条目全部删除并且加上Leader的日志。一旦附加日志 RPC 成功，那么Follower的日志就会和Leader保持一致，并且在接下来的任期里一直继续保持。 实现核心类图 核心方法requestVote() 如果候选人的term &lt; currentTerm，不给该候选人投票 如果当前节点没有投票或者投给了候选人并且候选人日志和当前节点一样新，就给该候选人投票 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * 节点选举 * 选举超时：Follower等待成为Leader的时间，随机设置在150ms ~ 300ms * @param request { * term: 候选人任期号 * candidateId: 候选人Id * lastLogIndex: 候选人最好的日志条目索引值 * lastLogTerm: 候选人最后日志条目的任期号 * } * ZRaftResponse { * &quot;term&quot;: 当前任期号 * &quot;voteGranted&quot;: true / false * 是否被投票 * } */public void requestVote(VoteRequest request, StreamObserver&lt;ZRaftResponse&gt; responseObserver) { // 当节点收到比自己大的任期，会将自己的任期设置为相同的，然后直接投票 // 当节点收到和自己一样大的任期，会看自己是否已经投票来判断 ZRaftResponse response = ZRaftResponse.newBuilder() .setTerm(NodeManager.node.getCurrentTerm()) .setSuccess(vote(request)) .build(); responseObserver.onNext(response); responseObserver.onCompleted();}/** * 判断当前节点是否投票给候选人 * 如果候选人的term &lt; currentTerm，不给该候选人投票 * 如果当前节点没有投票或者投给了候选人并且候选人日志和当前节点一样新，就给该候选人投票 * @param request 候选人id * @return true / false */private synchronized boolean vote(VoteRequest request) { long term = request.getTerm(); long currentTerm = NodeManager.node.getCurrentTerm(); if (term &lt; currentTerm) { // 如果请求者任期小于当前节点任期 return false; } if (term &gt; currentTerm) { // 更新等待定时器的时间 NodeManager.electionListener .updatePreHeartTime(System.currentTimeMillis()); // 修改节点任期信息 zRaftService.updateNodeTermInfo(request); return true; } long votedFor = NodeManager.node.getVotedFor(); long candidateId = request.getCandidateId(); if (votedFor == 0 || (votedFor == candidateId &amp;&amp; NodeManager.node.getLogIndex() == request.getLastLogIndex() &amp;&amp; NodeManager.node.getLastLogTerm() == request.getLastLogTerm())) { // 如果当前节点没有投票或者 // 给请求者投票了并且日志索引与任期能对应 // 更新等待定时器的时间 NodeManager.electionListener .updatePreHeartTime(System.currentTimeMillis()); NodeManager.node.setLeaderId(0); NodeManager.node.setVotedFor(candidateId); NodeManager.printNodeInfo(); return true; } return false;} appendEntries() Leader接收到数据更改，将更改添加到节点日志中（不提交） 将该条目复制到Follower，等待回复，直到大多数（n / 2 + 1）节点响应成功。如果没有超过半数的节点响应成功，隔段超时时间后重新发送。 Leader提交数据，然后将结果返回给并通知Follower进行提交 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495/** * 追加条目，心跳，节点间数据的同步，日志复制 * 1. Leader接收到数据更改，将更改添加到节点日志中（不提交） * 2. 将该条目复制到Follower，等待回复，直到大多数（n / 2 + 1） * 节点响应成功。如果没有超过半数的节点响应成功，隔段超时时间后重新发送 * 3. Leader提交数据，然后将结果返回给并通知Follower进行提交 * @param request { * term: Leader任期 * leaderId: 有时候可能是Candidate收到请求， * 需要将请求重定向到Leader去 * preLogIndex: 前一个日志条目的索引 * preLogTerm: 前一个日志条目的任期 * entries: 需要被保存的日志条目（如果为空，代表是心跳） * leaderCommit: Leader已提交的最高日志条目的索引 * } * ZRaftResponse { * &quot;term&quot;: 当前任期 * &quot;success&quot;: true / false。如果Candidate * 所含有的条目和prevLogIndex以及preLogTerm * 匹配上，则为true。 * } */@Overridepublic void appendEntries(AppendRequest request, StreamObserver&lt;ZRaftResponse&gt; responseObserver) { NodeManager.printLog(&quot;appendEntries...&quot;); ZRaftResponse.Builder builder = ZRaftResponse.newBuilder() .setTerm(NodeManager.node.getCurrentTerm()); // 如果currentTerm &gt; term long term = request.getTerm(); long currentTerm = NodeManager.node.getCurrentTerm(); if (term &lt; currentTerm) { // 返回false builder.setSuccess(false); responseObserver.onNext(builder.build()); responseObserver.onCompleted(); return; } // 更新等待计时器 NodeManager.electionListener .updatePreHeartTime(System.currentTimeMillis()); // 如果term &gt; currentTerm 或者当前节点状态是Candidate Node.NodeState state = NodeManager.node.getNodeState(); if (term &gt; currentTerm || state == Node.NodeState.CANDIDATE) { // 修改任期状态并切换为Follower zRaftService.levelDown(request); } else { // 设置LeaderId long leaderId = NodeManager.node.getLeaderId(); if (leaderId == 0) { NodeManager.node.setLeaderId(request.getLeaderId()); NodeManager.node.setNodeState(Node.NodeState.FOLLOWER); NodeManager.printNodeInfo(); } } long preLogTerm = request.getPreLogTerm(); long preLogIndex = request.getPreLogIndex(); // 如果Leader日志索引不能在当前节点的索引上找到 if (!NodeManager.node.entryIsExist(preLogTerm, preLogIndex)) { // 返回false builder.setSuccess(false); responseObserver.onNext(builder.build()); responseObserver.onCompleted(); return; } boolean b = true; // 如果不是心跳包 List&lt;Entry&gt; entries = request.getEntriesList(); if (entries.size() != 0) { // 添加日志条目 b = NodeManager.node.addLogEntries(preLogIndex, entries); NodeManager.printLog(NodeManager.node.toString()); } // 判断是否要提交条目 long leaderCommit = request.getLeaderCommit(); long commitIndex = NodeManager.node.getCommitIndex(); if (leaderCommit &gt; commitIndex) { // 将提交 b = NodeManager.node.commitLog(leaderCommit) &amp;&amp; b; } builder.setSuccess(b); responseObserver.onNext(builder.build()); responseObserver.onCompleted();} sendCommand()客户端调用的RPC方法。 如果当前节点是Leader: 第一阶段，将指令保存在log条目中，给其他节点发送AppendEntries，异步等待消息。 第二阶段，当大多数节点返回true，在本地进行提交并将结果返回给用户，同时向其他节点发送提交命令. 如果当前节点是Follower: 将该请求重定向到Leader去。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * 客户端调用的RPC方法。 * 如果当前节点是Leader: * 第一阶段，将指令保存在log条目中，给其他节点发送AppendEntries，异步等待消息。 * 第二阶段，当大多数节点返回true，在本地进行提交并将结果返回给用户，同时向其他节点 * 发送提交命令. * 如果当前节点是Follower: * 将该请求重定向到Leader去。 * @param request 指令集（字符串list） */@Overridepublic void sendCommand(Command request, StreamObserver&lt;ClientResponse&gt; responseObserver) { ProtocolStringList commandList = request.getCommandList(); ClientResponse.Builder builder = ClientResponse.newBuilder(); boolean b = false; int size = commandList.size(); Node.NodeState state = NodeManager.node.getNodeState(); long leaderId = NodeManager.node.getLeaderId(); if (size == 0 || state != Node.NodeState.LEADER) { // 如果用户没有发送条目或者当前节点不是Leader，直接返回false并添加LeaderId responseObserver.onNext(builder.setSuccess(b).setLeaderId(leaderId).build()); responseObserver.onCompleted(); return; } // 处理该请求 // 第一阶段，保存指令到本地并给其他节点发送消息 long currentTerm = NodeManager.node.getCurrentTerm(); List&lt;Entry&gt; entries = new ArrayList&lt;&gt;(); for (String command : commandList) { Entry entry = Entry.newBuilder() .setTerm(currentTerm) .setCommand(command) .build(); entries.add(entry); } if (!NodeManager.node.addLogEntries(entries)) { // 如果本地添加条目失败，返回false responseObserver.onNext(builder.setSuccess(b).setLeaderId(leaderId).build()); responseObserver.onCompleted(); return; } // 将返回交给AppendFutureListener AppendFutureListener.responseObserver = responseObserver; // 发送RPC请求 zRaftService.sendAppendEntries(1);} 参考资料Raft论文 Raft动态展示 MIT6.824","link":"/2021/11/03/%E9%80%9A%E8%BF%87Java%E5%AE%9E%E7%8E%B0Raft%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"源码","slug":"源码","link":"/tags/%E6%BA%90%E7%A0%81/"},{"name":"Collection","slug":"Collection","link":"/tags/Collection/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"icarus","slug":"icarus","link":"/tags/icarus/"},{"name":"配置","slug":"配置","link":"/tags/%E9%85%8D%E7%BD%AE/"},{"name":"线程","slug":"线程","link":"/tags/%E7%BA%BF%E7%A8%8B/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"TCP","slug":"TCP","link":"/tags/TCP/"},{"name":"Linux网络","slug":"Linux网络","link":"/tags/Linux%E7%BD%91%E7%BB%9C/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"比赛","slug":"比赛","link":"/tags/%E6%AF%94%E8%B5%9B/"},{"name":"复盘","slug":"复盘","link":"/tags/%E5%A4%8D%E7%9B%98/"},{"name":"天池","slug":"天池","link":"/tags/%E5%A4%A9%E6%B1%A0/"},{"name":"刷题","slug":"刷题","link":"/tags/%E5%88%B7%E9%A2%98/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"分布式","slug":"分布式","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"一致性协议","slug":"一致性协议","link":"/tags/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE/"},{"name":"实现","slug":"实现","link":"/tags/%E5%AE%9E%E7%8E%B0/"}],"categories":[{"name":"Code","slug":"Code","link":"/categories/Code/"},{"name":"Java","slug":"Code/Java","link":"/categories/Code/Java/"},{"name":"Linux","slug":"Code/Linux","link":"/categories/Code/Linux/"},{"name":"其他","slug":"Code/其他","link":"/categories/Code/%E5%85%B6%E4%BB%96/"},{"name":"Spring","slug":"Code/Spring","link":"/categories/Code/Spring/"},{"name":"复盘","slug":"Code/复盘","link":"/categories/Code/%E5%A4%8D%E7%9B%98/"},{"name":"Thread","slug":"Code/Java/Thread","link":"/categories/Code/Java/Thread/"},{"name":"刷题","slug":"Code/刷题","link":"/categories/Code/%E5%88%B7%E9%A2%98/"},{"name":"Collection","slug":"Code/Java/Collection","link":"/categories/Code/Java/Collection/"},{"name":"分布式","slug":"Code/分布式","link":"/categories/Code/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"网络","slug":"Code/Linux/网络","link":"/categories/Code/Linux/%E7%BD%91%E7%BB%9C/"},{"name":"比赛","slug":"Code/复盘/比赛","link":"/categories/Code/%E5%A4%8D%E7%9B%98/%E6%AF%94%E8%B5%9B/"},{"name":"协议","slug":"Code/分布式/协议","link":"/categories/Code/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%8D%8F%E8%AE%AE/"},{"name":"实现","slug":"Code/实现","link":"/categories/Code/%E5%AE%9E%E7%8E%B0/"},{"name":"算法","slug":"Code/实现/算法","link":"/categories/Code/%E5%AE%9E%E7%8E%B0/%E7%AE%97%E6%B3%95/"}]}